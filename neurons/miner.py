#!/usr/bin/env python3
"""
Bittensor Miner for Audio Processing Subnet
Handles transcription, TTS, and summarization tasks with enhanced logging
"""

import sys
import os
# Ensure we use the local template from this project, not from other projects
_project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)

# Load environment variables from .env file early (before other imports)
from dotenv import load_dotenv
from pathlib import Path
_env_file = Path(_project_root) / ".env"
if _env_file.exists():
    load_dotenv(_env_file)
    print(f"‚úÖ Loaded .env file from: {_env_file}")
else:
    load_dotenv()  # Try loading from current directory
    print(f"‚ÑπÔ∏è No .env file found at {_env_file}, using system environment variables")

import time
import typing
import bittensor as bt
import httpx
import asyncio
import json
from datetime import datetime
import threading

# Bittensor Miner Template:
from template.base.miner import BaseMinerNeuron
from template.protocol import AudioTask

# Add FastAPI imports for the endpoints
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import JSONResponse
import uvicorn
from pydantic import BaseModel
import typing
from typing import Dict, Any, Optional
# Response models for the endpoints
class TaskResponse(BaseModel):
    task_id: str
    task_type: str
    status: str
    input_file: Dict[str, Any]
    source_language: str
    priority: str


class MinerResponse(BaseModel):
    task_id: str
    miner_uid: int
    response_data: Dict[str, Any]
    processing_time: float
    accuracy_score: float
    speed_score: float

# Import base miner class which takes care of most of the boilerplate
from template.base.miner import BaseMinerNeuron

class Miner(BaseMinerNeuron):
    """
    Your miner neuron. The class name should be "Miner" and should inherit from BaseMinerNeuron.
    """

    def __init__(self, config=None):
        super(Miner, self).__init__(config=config)
        
        # Miner will query proxy server for tasks instead of running its own API server
        self.proxy_server_url = "https://violet-proxy-bl4w.onrender.com"  # Production proxy server URL
        self.last_task_query = 0
        self.task_query_interval = 10  # Query every 10 seconds
        
        # Load miner API key from environment variable (like HF_TOKEN)
        self.miner_api_key = os.getenv('MINER_API_KEY')
        if not self.miner_api_key:
            bt.logging.warning("‚ö†Ô∏è  MINER_API_KEY not found in .env file. Miner endpoints will be rejected.")
            bt.logging.warning("   Add MINER_API_KEY=<your_api_key> to your .env file")
        else:
            bt.logging.info(f"‚úÖ Miner API key loaded from .env (length: {len(self.miner_api_key)})")
        
        # Enhanced logging setup
        self.setup_enhanced_logging()
        
        # Record start time for uptime tracking
        self._start_time = time.time()
        
        bt.logging.info("Initializing pipeline manager for on-demand model loading...")
        
        # Initialize pipeline manager for dynamic model loading
        # Models will be loaded only when tasks are assigned
        # Import only the class definition, avoiding any module-level execution
        from template.pipelines.pipeline_manager import PipelineManager
        # Create a new instance - PipelineManager.__init__ only sets up empty dicts, no model loading
        self.pipeline_manager = PipelineManager()
        
        bt.logging.info("‚úÖ Pipeline manager initialized - models will be loaded on-demand when tasks are assigned")
        
        # Log configuration
        bt.logging.info(f"Axon IP: {self.config.axon.ip}")
        bt.logging.info(f"Axon Port: {self.config.axon.port}")
        bt.logging.info(f"Axon External IP: {self.config.axon.external_ip}")
        bt.logging.info(f"Axon External Port: {self.config.axon.external_port}")
        
        # Attach forward function to miner axon
        bt.logging.info("Attaching forward function to miner axon.")
        self.axon.attach(
            forward_fn=self.forward,
            blacklist_fn=self.blacklist,
            priority_fn=self.priority,
            verify_fn=self.verify
        )
        
        bt.logging.info(f"Axon created: {self.axon}")
        
        # Start background task to continuously query proxy server for tasks
        self.start_proxy_query_task()
        
        # Start periodic metrics saving
        self.start_periodic_metrics_saving()
        
        bt.logging.info("‚úÖ Miner initialization complete")
        
        # Initialize duplicate protection
        self.processed_tasks = set()  # Track processed task IDs
        self.processing_tasks = set()  # Track currently processing tasks
        self.max_processed_tasks = 1000  # Maximum tasks to keep in memory
        self.task_processing_lock = threading.Lock()  # Thread safety for task processing

    def setup_enhanced_logging(self):
        """Setup enhanced logging with structured logging and response tracking"""
        # Create logs directory if it doesn't exist
        self.logs_dir = Path("logs")
        self.logs_dir.mkdir(exist_ok=True)
        
        # Create miner-specific log directory
        self.miner_logs_dir = self.logs_dir / "miner"
        self.miner_logs_dir.mkdir(exist_ok=True)
        
        # Create response logs directory
        self.response_logs_dir = self.miner_logs_dir / "responses"
        self.response_logs_dir.mkdir(exist_ok=True)
        
        # Create metrics logs directory
        self.metrics_logs_dir = self.miner_logs_dir / "metrics"
        self.metrics_logs_dir.mkdir(exist_ok=True)
        
        # Initialize response tracking
        self.response_count = 0
        self.successful_responses = 0
        self.failed_responses = 0
        self.total_processing_time = 0.0
        
        bt.logging.info(f"üìÅ Enhanced logging setup complete")
        bt.logging.info(f"   Logs directory: {self.logs_dir.absolute()}")
        bt.logging.info(f"   Miner logs: {self.miner_logs_dir.absolute()}")
        bt.logging.info(f"   Response logs: {self.response_logs_dir.absolute()}")
        bt.logging.info(f"   Metrics logs: {self.metrics_logs_dir.absolute()}")

    def log_response(self, task_id: str, task_type: str, miner_uid: int, result: dict, 
                    processing_time: float, input_size: int, success: bool, error: str = None):
        """Log detailed response information"""
        try:
            timestamp = datetime.now().isoformat()
            response_id = f"{task_id}_{miner_uid}_{int(time.time())}"
            
            # Create response log entry
            response_log = {
                "response_id": response_id,
                "timestamp": timestamp,
                "task_id": task_id,
                "task_type": task_type,
                "miner_uid": miner_uid,
                "success": success,
                "processing_time": processing_time,
                "input_size": input_size,
                "result_summary": self._summarize_result(result, task_type),
                "error": error,
                "metrics": {
                    "accuracy_score": result.get("accuracy_score", 0.0) if success else 0.0,
                    "speed_score": result.get("speed_score", 0.0) if success else 0.0,
                    "confidence": result.get("confidence", 0.0) if success else 0.0
                }
            }
            
            # Save detailed response log
            response_log_file = self.response_logs_dir / f"{response_id}.json"
            with open(response_log_file, 'w') as f:
                json.dump(response_log, f, indent=2, default=str)
            
            # Update metrics
            self.response_count += 1
            if success:
                self.successful_responses += 1
            else:
                self.failed_responses += 1
            self.total_processing_time += processing_time
            
            # Log to console with enhanced formatting
            status_emoji = "‚úÖ" if success else "‚ùå"
            bt.logging.info(f"{status_emoji} RESPONSE LOGGED: {response_id}")
            bt.logging.info(f"   Task: {task_type} ({task_id})")
            bt.logging.info(f"   Miner: {miner_uid}")
            bt.logging.info(f"   Processing: {processing_time:.2f}s")
            bt.logging.info(f"   Input: {input_size} bytes")
            bt.logging.info(f"   Success: {success}")
            if error:
                bt.logging.error(f"   Error: {error}")
            
            # Log metrics summary
            self._log_metrics_summary()
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error logging response: {e}")

    def _summarize_result(self, result: dict, task_type: str) -> dict:
        """Create a summary of the result for logging"""
        try:
            summary = {
                "task_type": task_type,
                "has_error": "error" in result,
                "output_size": 0
            }
            
            if task_type == "transcription":
                summary["output_size"] = len(result.get("transcript", ""))
                summary["confidence"] = result.get("confidence", 0.0)
            elif task_type == "tts":
                summary["output_size"] = len(result.get("output_data", ""))
                summary["text_length"] = result.get("text_length", 0)
            elif task_type == "summarization":
                summary["output_size"] = len(result.get("summary", ""))
                summary["text_length"] = result.get("text_length", 0)
            
            return summary
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error summarizing result: {e}")
            return {"error": str(e)}

    def _log_metrics_summary(self):
        """Log current metrics summary"""
        try:
            if self.response_count > 0:
                success_rate = (self.successful_responses / self.response_count) * 100
                avg_processing_time = self.total_processing_time / self.response_count
                
                bt.logging.info(f"üìä METRICS SUMMARY:")
                bt.logging.info(f"   Total Responses: {self.response_count}")
                bt.logging.info(f"   Success Rate: {success_rate:.1f}%")
                bt.logging.info(f"   Avg Processing Time: {avg_processing_time:.2f}s")
                bt.logging.info(f"   Successful: {self.successful_responses}")
                bt.logging.info(f"   Failed: {self.failed_responses}")
                
                # Save metrics to file
                metrics_data = {
                    "timestamp": datetime.now().isoformat(),
                    "total_responses": self.response_count,
                    "successful_responses": self.successful_responses,
                    "failed_responses": self.failed_responses,
                    "success_rate": success_rate,
                    "total_processing_time": self.total_processing_time,
                    "average_processing_time": avg_processing_time
                }
                
                metrics_file = self.metrics_logs_dir / f"metrics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(metrics_file, 'w') as f:
                    json.dump(metrics_data, f, indent=2)
                    
        except Exception as e:
            bt.logging.error(f"‚ùå Error logging metrics summary: {e}")

    def save_comprehensive_metrics(self):
        """Save comprehensive metrics and create summary report"""
        try:
            timestamp = datetime.now()
            timestamp_str = timestamp.isoformat()
            date_str = timestamp.strftime('%Y%m%d_%H%M%S')
            
            # Create comprehensive metrics report
            comprehensive_metrics = {
                "timestamp": timestamp_str,
                "miner_info": {
                    "uid": getattr(self, 'uid', 0),
                    "hotkey": getattr(self, 'wallet', {}).get('hotkey', 'unknown') if hasattr(self, 'wallet') else 'unknown',
                    "coldkey": getattr(self, 'wallet', {}).get('coldkey', 'unknown') if hasattr(self, 'wallet') else 'unknown'
                },
                "performance_metrics": {
                    "total_responses": self.response_count,
                    "successful_responses": self.successful_responses,
                    "failed_responses": self.failed_responses,
                    "success_rate": (self.successful_responses / self.response_count * 100) if self.response_count > 0 else 0,
                    "total_processing_time": self.total_processing_time,
                    "average_processing_time": (self.total_processing_time / self.response_count) if self.response_count > 0 else 0,
                    "uptime": time.time() - getattr(self, '_start_time', time.time())
                },
                "pipeline_status": {
                    "pipeline_manager": "ready",
                    "models_loaded_on_demand": True,
                    "cache_stats": self.pipeline_manager.get_cache_stats()
                },
                "system_info": {
                    "proxy_server_url": self.proxy_server_url,
                    "task_query_interval": self.task_query_interval,
                    "logs_directory": str(self.logs_dir.absolute())
                }
            }
            
            # Save comprehensive metrics
            comprehensive_file = self.metrics_logs_dir / f"comprehensive_metrics_{date_str}.json"
            with open(comprehensive_file, 'w') as f:
                json.dump(comprehensive_metrics, f, indent=2, default=str)
            
            # Create human-readable summary report
            summary_report = f"""# Miner Performance Summary Report
Generated: {timestamp_str}

## Miner Information
- UID: {comprehensive_metrics['miner_info']['uid']}
- Hotkey: {comprehensive_metrics['miner_info']['hotkey']}
- Coldkey: {comprehensive_metrics['miner_info']['coldkey']}

## Performance Metrics
- Total Responses: {comprehensive_metrics['performance_metrics']['total_responses']}
- Success Rate: {comprehensive_metrics['performance_metrics']['success_rate']:.1f}%
- Average Processing Time: {comprehensive_metrics['performance_metrics']['average_processing_time']:.2f}s
- Total Processing Time: {comprehensive_metrics['performance_metrics']['total_processing_time']:.2f}s

## Pipeline Status
- Transcription Pipeline: {'‚úÖ Available' if comprehensive_metrics['pipeline_status']['transcription_pipeline'] else '‚ùå Not Available'}
- TTS Pipeline: {'‚úÖ Available' if comprehensive_metrics['pipeline_status']['tts_pipeline'] else '‚ùå Not Available'}
- Summarization Pipeline: {'‚úÖ Available' if comprehensive_metrics['pipeline_status']['summarization_pipeline'] else '‚ùå Not Available'}

## System Configuration
- Proxy Server: {comprehensive_metrics['system_info']['proxy_server_url']}
- Task Query Interval: {comprehensive_metrics['system_info']['task_query_interval']}s
- Logs Directory: {comprehensive_metrics['system_info']['logs_directory']}

---
Report generated automatically by Bittensor Miner
"""
            
            # Save summary report
            summary_file = self.metrics_logs_dir / f"summary_report_{date_str}.md"
            with open(summary_file, 'w') as f:
                f.write(summary_report)
            
            bt.logging.info(f"üìä Comprehensive metrics saved: {comprehensive_file}")
            bt.logging.info(f"üìã Summary report saved: {summary_file}")
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error saving comprehensive metrics: {e}")

    def start_periodic_metrics_saving(self):
        """Start periodic saving of comprehensive metrics"""
        import threading
        
        def metrics_saving_loop():
            while True:
                try:
                    time.sleep(300)  # Save metrics every 5 minutes
                    self.save_comprehensive_metrics()
                except Exception as e:
                    bt.logging.error(f"‚ùå Error in metrics saving loop: {e}")
        
        # Start metrics saving thread
        metrics_thread = threading.Thread(target=metrics_saving_loop, daemon=True)
        metrics_thread.start()
        bt.logging.info("üìä Started periodic metrics saving (every 5 minutes)")

    def start_periodic_metrics_saving(self):
        """Start background task to periodically save metrics"""
        import threading
        import time
        
        def metrics_saving_loop():
            while True:
                try:
                    # Save metrics every 5 minutes
                    time.sleep(300)  # 5 minutes
                    if hasattr(self, 'miner_logs_dir'):
                        self._save_metrics()
                except Exception as e:
                    bt.logging.warning(f"‚ö†Ô∏è Metrics saving error: {e}")
        
        # Start background thread
        metrics_thread = threading.Thread(target=metrics_saving_loop, daemon=True)
        metrics_thread.start()
        bt.logging.info("üîÑ Started periodic metrics saving (every 5 minutes)")
    
    def _save_metrics(self):
        """Save current metrics to file"""
        try:
            metrics = {
                'timestamp': datetime.now().isoformat(),
                'response_count': getattr(self, 'response_count', 0),
                'successful_responses': getattr(self, 'successful_responses', 0),
                'failed_responses': getattr(self, 'failed_responses', 0),
                'total_processing_time': getattr(self, 'total_processing_time', 0.0),
                'success_rate': getattr(self, 'successful_responses', 0) / max(getattr(self, 'response_count', 1), 1),
                'avg_processing_time': getattr(self, 'total_processing_time', 0.0) / max(getattr(self, 'response_count', 1), 1)
            }
            
            metrics_file = self.metrics_logs_dir / f"metrics_{int(time.time())}.json"
            with open(metrics_file, 'w') as f:
                json.dump(metrics, f, indent=2, default=str)
            
            bt.logging.debug(f"üìä Metrics saved to {metrics_file}")
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error saving metrics: {e}")

    def log_task_start(self, task_id: str, task_type: str, miner_uid: int, input_size: int):
        """Log when a task starts processing"""
        try:
            timestamp = datetime.now().isoformat()
            start_log = {
                "timestamp": timestamp,
                "event": "task_start",
                "task_id": task_id,
                "task_type": task_type,
                "miner_uid": miner_uid,
                "input_size": input_size
            }
            
            # Save start log
            start_log_file = self.response_logs_dir / f"{task_id}_{miner_uid}_start.json"
            with open(start_log_file, 'w') as f:
                json.dump(start_log, f, indent=2)
            
            bt.logging.info(f"üöÄ TASK START: {task_id} ({task_type}) - Miner {miner_uid}")
            bt.logging.info(f"   Input Size: {input_size} bytes")
            bt.logging.info(f"   Start Time: {timestamp}")
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error logging task start: {e}")

    def log_task_completion(self, task_id: str, task_type: str, miner_uid: int, 
                           processing_time: float, success: bool, result: dict, error: str = None):
        """Log when a task completes processing"""
        try:
            timestamp = datetime.now().isoformat()
            completion_log = {
                "timestamp": timestamp,
                "event": "task_completion",
                "task_id": task_id,
                "task_type": task_type,
                "miner_uid": miner_uid,
                "processing_time": processing_time,
                "success": success,
                "result_summary": self._summarize_result(result, task_type),
                "error": error
            }
            
            # Save completion log
            completion_log_file = self.response_logs_dir / f"{task_id}_{miner_uid}_completion.json"
            with open(completion_log_file, 'w') as f:
                json.dump(completion_log, f, indent=2)
            
            status_emoji = "‚úÖ" if success else "‚ùå"
            bt.logging.info(f"{status_emoji} TASK COMPLETION: {task_id} ({task_type}) - Miner {miner_uid}")
            bt.logging.info(f"   Processing Time: {processing_time:.2f}s")
            bt.logging.info(f"   Success: {success}")
            if error:
                bt.logging.error(f"   Error: {error}")
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error logging task completion: {e}")
    
    async def query_proxy_for_tasks(self):
        """Query proxy server for tasks assigned to this miner"""
        # Get miner UID from Bittensor
        miner_uid = self.uid if hasattr(self, 'uid') else 0
        
        if miner_uid == 0:
            bt.logging.debug("üîÑ Miner UID not available yet, skipping task query")
            return
        
        bt.logging.info(f"üîç Miner {miner_uid} querying proxy server for assigned tasks...")
        
        # üîí DUPLICATE PROTECTION: Enhanced task filtering
        # Only query for "assigned" tasks to avoid processing tasks (reduces logging)
        # Fix for anyio circular import issue - pre-initialize anyio
        try:
            # Pre-initialize anyio to avoid circular import issues
            import anyio
            # Access anyio module to force initialization (don't check __version__ as it may not exist)
            _ = anyio
        except (ImportError, AttributeError):
            pass  # Ignore anyio initialization errors
        
        # Create httpx client with explicit timeout and limits to avoid circular import
        client = None
        try:
            client = httpx.AsyncClient(
                timeout=httpx.Timeout(30.0, connect=10.0),
                limits=httpx.Limits(max_keepalive_connections=5, max_connections=10)
            )
        except Exception as client_error:
            # Fallback: try with minimal configuration
            bt.logging.warning(f"‚ö†Ô∏è Error creating httpx client with full config: {client_error}")
            try:
                client = httpx.AsyncClient(timeout=30.0)
            except Exception as fallback_error:
                bt.logging.error(f"‚ùå Failed to create httpx client: {fallback_error}")
                bt.logging.error(f"   This may be due to anyio/httpx version conflicts")
                bt.logging.error(f"   Try: pip install --upgrade httpx anyio")
                return
        
        try:
            headers = self._get_auth_headers()
            
            if not headers.get("X-API-Key"):
                bt.logging.error("‚ùå No API key found! Miner cannot authenticate with proxy server.")
                bt.logging.error("   Set MINER_API_KEY in .env file")
                return
            
            # Only query for assigned tasks
            try:
                response = await client.get(
                    f"{self.proxy_server_url}/api/v1/miners/{miner_uid}/tasks",
                    headers=headers,
                    params={"status": "assigned"},
                    timeout=30.0
                )
                
                if response.status_code == 200:
                    tasks = response.json()
                    if tasks and len(tasks) > 0:
                        bt.logging.info(f"üéØ Found {len(tasks)} assigned tasks for miner {miner_uid}")
                        
                        # üîí DUPLICATE PROTECTION: Additional filtering before processing
                        eligible_tasks = []
                        for task in tasks:
                            task_id = task.get("task_id")
                            task_status = task.get("status")
                            
                            # Skip if already processed
                            if task_id in self.processed_tasks:
                                bt.logging.debug(f"üîÑ Skipping already processed task: {task_id}")
                                continue
                            
                            # Skip if currently being processed
                            if task_id in self.processing_tasks:
                                bt.logging.debug(f"‚è≥ Skipping currently processing task: {task_id}")
                                continue
                            
                            # Only accept assigned or pending tasks (exclude processing to reduce logging)
                            if task_status not in ['assigned', 'pending']:
                                bt.logging.debug(f"‚ö†Ô∏è Skipping task with invalid status '{task_status}': {task_id}")
                                continue
                            
                            eligible_tasks.append(task)
                        
                        # Process eligible tasks after filtering (fixed indentation)
                        if len(eligible_tasks) > 0:
                            bt.logging.info(f"‚úÖ {len(eligible_tasks)} assigned tasks eligible for processing")
                            
                            # Process each eligible task
                            for task in eligible_tasks:
                                await self.process_proxy_task(task)
                        else:
                            bt.logging.debug(f"üîÑ No eligible tasks after filtering")
                    else:
                        bt.logging.debug(f"üîÑ No assigned tasks for miner {miner_uid}")
                else:
                    bt.logging.warning(f"‚ö†Ô∏è Failed to query assigned tasks: {response.status_code}")
            except Exception as e:
                error_msg = str(e)
                if "CancelScope" in error_msg or "anyio" in error_msg.lower() or "partially initialized" in error_msg.lower():
                    bt.logging.warning(f"‚ö†Ô∏è Error querying assigned tasks (anyio/httpx issue): {e}")
                    bt.logging.warning(f"   This is a known httpx/anyio version conflict")
                    bt.logging.warning(f"   Solution: pip install --upgrade 'httpx>=0.25.0,<0.28.0' 'anyio>=4.0.0,<5.0.0'")
                else:
                    bt.logging.warning(f"‚ö†Ô∏è Error querying assigned tasks: {e}")
        except Exception as e:
            error_msg = str(e)
            if "CancelScope" in error_msg or "anyio" in error_msg.lower() or "partially initialized" in error_msg.lower():
                bt.logging.warning(f"‚ö†Ô∏è Error querying proxy for tasks (anyio/httpx issue): {e}")
                bt.logging.warning(f"   This is a known httpx/anyio version conflict")
                bt.logging.warning(f"   Solution: pip install --upgrade 'httpx>=0.25.0,<0.28.0' 'anyio>=4.0.0,<5.0.0'")
            else:
                bt.logging.warning(f"‚ö†Ô∏è Error querying proxy for tasks: {e}")
    
    def start_proxy_query_task(self):
        """Start background task to continuously query proxy server for tasks"""
        import threading
        import time
        
        def proxy_query_loop():
            while True:
                try:
                    # Use asyncio.run to call the async method from a separate thread
                    import asyncio
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        # Query for tasks
                        loop.run_until_complete(self.query_proxy_for_tasks())
                        
                        # üîí DUPLICATE PROTECTION: Clean up old processed tasks
                        self.cleanup_processed_tasks()
                        
                    finally:
                        loop.close()
                except Exception as e:
                    bt.logging.warning(f"‚ö†Ô∏è Background proxy query error: {e}")
                
                # Wait before next query
                time.sleep(self.task_query_interval)
        
        # Start background thread
        proxy_thread = threading.Thread(target=proxy_query_loop, daemon=True)
        proxy_thread.start()
        bt.logging.info(f"üîÑ Started background proxy query task (every {self.task_query_interval} seconds)")
    
    async def test_file_download(self):
        """Test file download capability - only tests if there are actual tasks with files"""
        try:
            bt.logging.info("üß™ Testing file download capability from proxy server...")
            
            # Instead of using a hardcoded file ID, check if there are any tasks with files
            # This makes the test dynamic and only runs when there's actual data to test
            miner_uid = self.uid if hasattr(self, 'uid') else None
            if not miner_uid:
                bt.logging.info("‚ÑπÔ∏è Skipping file download test - miner UID not available yet")
                return
            
            # Query for assigned tasks to find a real file to test with
            try:
                async with httpx.AsyncClient(timeout=5.0) as client:
                    headers = self._get_auth_headers()
                    response = await client.get(
                        f"{self.proxy_server_url}/api/v1/miners/{miner_uid}/tasks",
                        headers=headers
                    )
                    
                    if response.status_code == 200:
                        tasks = response.json()
                        if isinstance(tasks, dict):
                            tasks = tasks.get('tasks', [])
                        
                        # Find a task with a file ID
                        test_file_id = None
                        for task in tasks:
                            # Check various possible file ID locations
                            if 'input_file_id' in task:
                                test_file_id = task['input_file_id']
                                break
                            elif 'input_file' in task and isinstance(task['input_file'], dict):
                                test_file_id = task['input_file'].get('file_id')
                                if test_file_id:
                                    break
                            elif 'file_id' in task:
                                test_file_id = task['file_id']
                                break
                        
                        if test_file_id:
                            bt.logging.info(f"üß™ Testing download with real task file: {test_file_id[:20]}...")
                            test_url = f"{self.proxy_server_url}/api/v1/files/{test_file_id}/download"
                            
                            # Use our download function
                            downloaded_data = await self.download_file_from_proxy(test_url)
                            
                            if downloaded_data is not None:
                                bt.logging.info(f"‚úÖ File download test PASSED - downloaded {len(downloaded_data)} bytes")
                                
                                # Test if it's valid audio data
                                if len(downloaded_data) > 1000:  # Should be at least 1KB
                                    bt.logging.info("‚úÖ File download test PASSED - file size is valid")
                                    
                                    # Test if we can process it (pipeline loads on-demand)
                                    try:
                                        result = await self.process_transcription_task(downloaded_data)
                                        if result and "error" not in result:
                                            bt.logging.info("‚úÖ Transcription pipeline test PASSED")
                                            bt.logging.info(f"   Transcript: {result.get('transcript', '')[:100]}...")
                                        else:
                                            bt.logging.debug("‚ÑπÔ∏è Transcription pipeline test - pipeline returned error (this is OK for testing)")
                                    except Exception as e:
                                        bt.logging.debug(f"‚ÑπÔ∏è Transcription pipeline test - {e} (this is OK for testing)")
                                    else:
                                        bt.logging.info("‚ÑπÔ∏è Transcription pipeline not available, skipping pipeline test")
                                else:
                                    bt.logging.debug("‚ÑπÔ∏è File download test - file seems too small (this is OK for testing)")
                            else:
                                bt.logging.debug("‚ÑπÔ∏è File download test - no data received (file may not exist, this is OK)")
                        else:
                            bt.logging.info("‚ÑπÔ∏è No tasks with files found - skipping file download test (this is normal)")
                    else:
                        bt.logging.debug(f"‚ÑπÔ∏è Could not fetch tasks for testing: {response.status_code} (this is OK)")
            except Exception as e:
                bt.logging.debug(f"‚ÑπÔ∏è File download test skipped: {e} (this is OK)")
                    
        except Exception as e:
            # Don't log as error - this is just a test and failures are expected
            bt.logging.debug(f"‚ÑπÔ∏è File download test skipped: {e} (this is OK)")

    async def test_miner_tasks_endpoint(self):
        """Test the miner tasks endpoint to see what data structure is returned"""
        try:
            bt.logging.info("üß™ Testing miner tasks endpoint...")
            
            # Get miner UID
            miner_uid = self.uid if hasattr(self, 'uid') else 48  # Use 48 as fallback for testing
            
            bt.logging.info(f"üß™ Testing endpoint for miner {miner_uid}")
            
            # Test the endpoint with "assigned" status (tasks assigned to this miner)
            async with httpx.AsyncClient(timeout=10.0) as client:
                headers = self._get_auth_headers()
                response = await client.get(
                    f"{self.proxy_server_url}/api/v1/miners/{miner_uid}/tasks",
                    headers=headers,
                    params={"status": "assigned"}  # Correct: Miner should process assigned tasks
                )
                
                bt.logging.info(f"üß™ Response status: {response.status_code}")
                bt.logging.info(f"üß™ Response headers: {dict(response.headers)}")
                
                if response.status_code == 200:
                    tasks = response.json()
                    bt.logging.info(f"üß™ Response data type: {type(tasks)}")
                    bt.logging.info(f"üß™ Response data: {tasks}")
                    
                    if isinstance(tasks, list) and len(tasks) > 0:
                        bt.logging.info(f"üß™ Found {len(tasks)} tasks")
                        for i, task in enumerate(tasks):
                            bt.logging.info(f"üß™ Task {i+1}:")
                            bt.logging.info(f"   Task ID: {task.get('task_id', 'N/A')}")
                            bt.logging.info(f"   Task Type: {task.get('task_type', 'N/A')}")
                            bt.logging.info(f"   Status: {task.get('status', 'N/A')}")
                            bt.logging.info(f"   Available fields: {list(task.keys())}")
                            
                            # Check for input file related fields
                            input_fields = [k for k in task.keys() if 'file' in k.lower() or 'input' in k.lower()]
                            if input_fields:
                                bt.logging.info(f"   Input-related fields: {input_fields}")
                                for field in input_fields:
                                    bt.logging.info(f"      {field}: {task[field]}")
                            else:
                                bt.logging.warning(f"   ‚ö†Ô∏è No input-related fields found")
                    else:
                        bt.logging.info(f"üß™ No tasks found or empty response")
                else:
                    bt.logging.error(f"üß™ Endpoint returned error: {response.status_code}")
                    bt.logging.error(f"üß™ Error response: {response.text}")
                    
        except Exception as e:
            bt.logging.error(f"‚ùå Error testing miner tasks endpoint: {e}")
            import traceback
            traceback.print_exc()

    async def process_proxy_task(self, task_data: dict):
        """Process a task received from proxy server"""
        try:
            bt.logging.info(f"üîç Processing proxy task with data: {task_data}")
            
            task_id = task_data.get("task_id")
            task_type = task_data.get("task_type")
            task_status = task_data.get("status")
            
            # üîí DUPLICATE PROTECTION: Atomic task status checking and assignment
            with self.task_processing_lock:
                # Check if task already processed
                if task_id in self.processed_tasks:
                    bt.logging.info(f"üîÑ Task {task_id} already processed, skipping duplicate")
                    return
                
                # Check if task is currently being processed
                if task_id in self.processing_tasks:
                    bt.logging.info(f"‚è≥ Task {task_id} is currently being processed, skipping duplicate")
                    return
                
                # Check if task status is valid for processing
                if task_status not in ['assigned', 'pending']:
                    bt.logging.info(f"‚ö†Ô∏è Task {task_id} has status '{task_status}', not eligible for processing")
                    return
                
                # Mark task as currently being processed (atomic operation)
                self.processing_tasks.add(task_id)
                bt.logging.info(f"üîí Task {task_id} locked for processing")
            
            try:
                # For summarization tasks, we handle text-based input differently
                if task_type == "summarization":
                    await self.process_summarization_task_from_proxy(task_data)
                    return
                
                # For TTS tasks, we handle text-based input differently
                if task_type == "tts":
                    await self.process_tts_task_from_proxy(task_data)
                    return
                
                # For text translation tasks, we handle text-based input differently
                if task_type == "text_translation":
                    await self.process_text_translation_task(task_data)
                    return
                
                # For document translation tasks, we handle file-based input differently
                if task_type == "document_translation":
                    await self.process_document_translation_task_from_proxy(task_data)
                    return
                
                # Validate task type
                supported_types = ["transcription", "tts", "summarization", "video_transcription", "text_translation", "document_translation"]
                if task_type not in supported_types:
                    bt.logging.error(f"‚ùå Unsupported task type: {task_type}. Supported types: {supported_types}")
                    return
                
                bt.logging.info(f"üéØ Processing proxy task {task_id} of type {task_type}")
                bt.logging.info(f"   Task data structure: {list(task_data.keys())}")
                
                # For transcription tasks, download directly from R2 URL (no base64)
                input_data = None
                input_size = 0
                
                if task_type == "transcription":
                    # Get R2 URL from task data or API
                    audio_url = None
                    if 'input_file' in task_data and isinstance(task_data['input_file'], dict):
                        input_file = task_data['input_file']
                        if input_file.get('storage_location') == 'r2':
                            audio_url = input_file.get('public_url')
                    
                    # If not in task data, fetch from proxy API
                    if not audio_url:
                        bt.logging.info(f"üì° Fetching audio URL from proxy API for task {task_id}")
                        try:
                            async with httpx.AsyncClient(timeout=60.0) as client:
                                headers = self._get_auth_headers()
                                response = await client.get(
                                    f"{self.proxy_server_url}/api/v1/miner/transcription/{task_id}",
                                    headers=headers
                                )
                                response.raise_for_status()
                                response_data = response.json()
                                
                                if response_data.get("success") and "audio_url" in response_data:
                                    audio_url = response_data["audio_url"]
                                    bt.logging.info(f"‚úÖ Got audio URL from API: {audio_url[:50]}...")
                                else:
                                    bt.logging.error(f"‚ùå No audio_url in API response for task {task_id}")
                                    return
                        except Exception as e:
                            bt.logging.error(f"‚ùå Failed to fetch audio URL from API: {e}")
                            return
                    
                    # Download audio directly from R2 URL (no base64)
                    if audio_url:
                        bt.logging.info(f"üåê Downloading audio directly from R2 URL for task {task_id}")
                        try:
                            async with httpx.AsyncClient(timeout=120.0) as client:
                                response = await client.get(audio_url)
                                response.raise_for_status()
                                audio_bytes = response.content
                                input_size = len(audio_bytes)
                                
                                if input_size == 0:
                                    bt.logging.error(f"‚ùå Downloaded audio is empty for task {task_id}")
                                    return
                                
                                bt.logging.info(f"‚úÖ Downloaded {input_size} bytes from R2 for task {task_id}")
                                
                                # Save to temporary file for processing
                                temp_wav_path = None
                                try:
                                    import librosa
                                    import soundfile as sf
                                    import io
                                    import tempfile
                                    import os
                                    
                                    # Load audio from bytes
                                    audio_io = io.BytesIO(audio_bytes)
                                    audio_array, sample_rate = librosa.load(audio_io, sr=None)
                                    
                                    # Create temporary WAV file
                                    with tempfile.NamedTemporaryFile(delete=False, suffix='.wav') as temp_file:
                                        temp_wav_path = temp_file.name
                                    
                                    # Save as WAV
                                    sf.write(temp_wav_path, audio_array, sample_rate)
                                    bt.logging.info(f"üíæ Saved audio to temporary file: {temp_wav_path} (sr={sample_rate}Hz)")
                                    
                                    # Read the temp file as bytes for processing
                                    with open(temp_wav_path, 'rb') as f:
                                        input_data = f.read()
                                    
                                    bt.logging.info(f"‚úÖ Audio ready for processing: {len(input_data)} bytes")
                                    
                                finally:
                                    # Clean up temporary file after processing
                                    if temp_wav_path and os.path.exists(temp_wav_path):
                                        try:
                                            os.unlink(temp_wav_path)
                                            bt.logging.debug(f"üßπ Cleaned up temporary file: {temp_wav_path}")
                                        except Exception as e:
                                            bt.logging.warning(f"‚ö†Ô∏è Failed to delete temporary file {temp_wav_path}: {e}")
                        except Exception as e:
                            bt.logging.error(f"‚ùå Failed to download audio from R2 URL: {e}")
                            return
                    else:
                        bt.logging.error(f"‚ùå No audio URL found for task {task_id}")
                        return
                
                # Fallback to file download for non-transcription tasks or if base64 not available
                if input_data is None:
                    # Handle multiple possible input file ID formats for file-based tasks
                    input_file_id = None
                    
                    # Try different possible field names and formats
                    possible_fields = [
                        "input_file_id",           # Direct field
                        "input_file",              # Object field
                        "file_id",                 # Alternative field name
                        "audio_file_id",           # Audio-specific field
                        "input_data_id"            # Another alternative
                    ]
                    
                    for field in possible_fields:
                        if field in task_data:
                            field_value = task_data[field]
                            if isinstance(field_value, str) and field_value:
                                input_file_id = field_value
                                bt.logging.info(f"üìã Found input_file_id in field '{field}': {input_file_id}")
                                break
                            elif isinstance(field_value, dict) and field_value.get('file_id'):
                                input_file_id = field_value['file_id']
                                bt.logging.info(f"üìã Found input_file_id in field '{field}.file_id': {input_file_id}")
                                break
                    
                    # If still no input_file_id, try to extract from nested structures
                    if not input_file_id:
                        # Check if there's a nested input structure
                        for key, value in task_data.items():
                            if isinstance(value, dict) and 'file_id' in value:
                                input_file_id = value['file_id']
                                bt.logging.info(f"üìã Found input_file_id in nested field '{key}.file_id': {input_file_id}")
                                break
                            elif isinstance(value, dict) and 'input_file_id' in value:
                                input_file_id = value['input_file_id']
                                bt.logging.info(f"üìã Found input_file_id in nested field '{key}.input_file_id': {input_file_id}")
                                break
                    
                    if not input_file_id:
                        bt.logging.error(f"‚ùå Missing required fields in task data:")
                        bt.logging.error(f"   Task ID: {task_id}")
                        bt.logging.error(f"   Task Type: {task_type}")
                        bt.logging.error(f"   Input File ID: {input_file_id}")
                        bt.logging.error(f"   Available fields: {list(task_data.keys())}")
                        return
                    
                    bt.logging.info(f"üì• Downloading input file from proxy: {input_file_id}")
                    
                    # Download input file from proxy
                    input_data = await self.download_file_from_proxy(f"{self.proxy_server_url}/api/v1/files/{input_file_id}/download")
                    
                    if input_data is None:
                        bt.logging.error(f"‚ùå Failed to download input file for task {task_id}")
                        return
                    
                    # Validate input data is not empty
                    input_size = len(input_data) if hasattr(input_data, '__len__') else 0
                    if input_size == 0:
                        bt.logging.warning(f"‚ö†Ô∏è Downloaded file is empty (0 bytes) for task {task_id} - marking as completed with broken file notice")
                        
                        # Instead of failing, create a completed response for broken files
                        broken_file_result = {
                            "transcript": "broken file",
                            "confidence": 0.0,
                            "processing_time": 0.0,
                            "language": "en",
                            "error": "File is empty or broken (0 bytes)",
                            "status": "completed_broken_file"
                        }
                        
                        # Log the broken file completion
                        miner_uid = self.uid if hasattr(self, 'uid') else 0
                        self.log_task_completion(task_id, task_type, miner_uid, 0.0, True, broken_file_result, "File marked as completed due to being broken/empty")
                        
                        # Submit the broken file result to proxy
                        await self.submit_result_to_proxy(f"{self.proxy_server_url}/api/v1/miner/response", task_id, broken_file_result)
                        
                        # Log the response
                        self.log_response(task_id, task_type, miner_uid, broken_file_result, 0.0, 0, True, "Broken file handled gracefully")
                        
                        bt.logging.info(f"‚úÖ Task {task_id} marked as completed (broken file) and submitted to proxy")
                        return
                    
                    # Check for suspiciously small files that might be corrupted
                    if input_size < 1000:  # Less than 1KB is suspicious for audio files
                        bt.logging.warning(f"‚ö†Ô∏è File is suspiciously small ({input_size} bytes) for task {task_id} - may be corrupted")
                        if task_type == "transcription":
                            bt.logging.warning(f"‚ö†Ô∏è Audio files should typically be larger than 1KB - marking as completed with broken file notice")
                            
                            # Create a completed response for suspiciously small files
                            suspicious_file_result = {
                                "transcript": "broken file",
                                "confidence": 0.0,
                                "processing_time": 0.0,
                                "language": "en",
                                "error": f"File is suspiciously small ({input_size} bytes) - may be corrupted",
                                "status": "completed_broken_file"
                            }
                            
                            # Log the suspicious file completion
                            miner_uid = self.uid if hasattr(self, 'uid') else 0
                            self.log_task_completion(task_id, task_type, miner_uid, 0.0, True, suspicious_file_result, "File marked as completed due to being suspiciously small")
                            
                            # Submit the suspicious file result to proxy
                            await self.submit_result_to_proxy(f"{self.proxy_server_url}/api/v1/miner/response", task_id, suspicious_file_result)
                            
                            # Log the response
                            self.log_response(task_id, task_type, miner_uid, suspicious_file_result, 0.0, input_size, True, "Suspiciously small file handled gracefully")
                            
                            bt.logging.info(f"‚úÖ Task {task_id} marked as completed (suspicious file) and submitted to proxy")
                            return
                    
                    bt.logging.info(f"üì• Downloaded {input_size} bytes for task {task_id}")
                
                # Validate input data type
                if task_type == "transcription" and not isinstance(input_data, bytes):
                    bt.logging.error(f"‚ùå Invalid input data type for transcription task {task_id}: expected bytes, got {type(input_data)}")
                    return
                
                # Get miner UID for logging
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                
                # Log task start
                self.log_task_start(task_id, task_type, miner_uid, input_size)
                
                # Process task using existing pipeline
                bt.logging.info(f"üîÑ Routing task {task_id} to {task_type} pipeline...")
                
                # Pipeline availability will be checked when loading models on-demand
                # No need to check here since models load when tasks are assigned
                
                # Get model_id and language from task if specified
                model_id = task_data.get("model_id")
                source_language = task_data.get("source_language", "en")
                target_language = task_data.get("target_language")
                
                if model_id:
                    bt.logging.info(f"üì¶ Task {task_id} specifies model: {model_id}")
                if source_language:
                    bt.logging.info(f"üåê Task {task_id} specifies source language: {source_language}")
                if target_language:
                    bt.logging.info(f"üåê Task {task_id} specifies target language: {target_language}")
                
                # Process task using appropriate pipeline
                start_time = time.time()
                try:
                    if task_type == "transcription":
                        result = await self.process_transcription_task(input_data, model_id=model_id, language=source_language)
                    elif task_type == "tts":
                        result = await self.process_tts_task(input_data, model_id=model_id, language=source_language)
                    elif task_type == "summarization":
                        result = await self.process_summarization_task(input_data, model_id=model_id, language=source_language)
                    elif task_type == "video_transcription":
                        result = await self.process_video_transcription_task(input_data, task_data, model_id=model_id, language=source_language)
                    elif task_type == "text_translation":
                        result = await self.process_text_translation_task(task_data, model_id=model_id, source_language=source_language, target_language=target_language)
                    elif task_type == "document_translation":
                        result = await self.process_document_translation_task(input_data, task_data, model_id=model_id, source_language=source_language, target_language=target_language)
                    else:
                        error_msg = f"Unknown task type: {task_type}"
                        bt.logging.error(f"‚ùå {error_msg}")
                        self.log_task_completion(task_id, task_type, miner_uid, 0.0, False, {}, error_msg)
                        return
                    
                    processing_time = time.time() - start_time
                    
                    # Validate result before submission
                    if not result or "error" in result:
                        error_msg = f"Task {task_id} failed to produce valid result: {result}"
                        bt.logging.error(f"‚ùå {error_msg}")
                        self.log_task_completion(task_id, task_type, miner_uid, processing_time, False, result, error_msg)
                        return
                    
                    # Log successful task completion
                    self.log_task_completion(task_id, task_type, miner_uid, processing_time, True, result)
                    
                    bt.logging.info(f"‚úÖ Task {task_id} processed successfully by {task_type} pipeline in {processing_time:.2f}s")
                    
                    # Submit result back to proxy
                    await self.submit_result_to_proxy(f"{self.proxy_server_url}/api/v1/miner/response", task_id, result)
                    
                    # Log final response
                    self.log_response(task_id, task_type, miner_uid, result, processing_time, input_size, True)
                    
                    bt.logging.info(f"‚úÖ Task {task_id} completed and result submitted")
                    
                except Exception as e:
                    processing_time = time.time() - start_time
                    error_msg = f"Pipeline processing error: {str(e)}"
                    bt.logging.error(f"‚ùå {error_msg}")
                    self.log_task_completion(task_id, task_type, miner_uid, processing_time, False, {}, error_msg)
                    self.log_response(task_id, task_type, miner_uid, {}, processing_time, input_size, False, error_msg)
                    return
                
            finally:
                # üîí DUPLICATE PROTECTION: Atomic task completion marking
                with self.task_processing_lock:
                    self.processing_tasks.discard(task_id)
                    self.processed_tasks.add(task_id)
                    bt.logging.info(f"‚úÖ Task {task_id} marked as processed, duplicate protection active")
                
        except Exception as e:
            # üîí DUPLICATE PROTECTION: Ensure task is removed from processing even on error
            task_id = task_data.get("task_id")
            if task_id:
                with self.task_processing_lock:
                    self.processing_tasks.discard(task_id)
                bt.logging.error(f"‚ùå Error processing task {task_id}: {e}")
            else:
                bt.logging.error(f"‚ùå Error processing task: {e}")

    def _get_auth_headers(self) -> dict:
        """Get authentication headers with API key"""
        headers = {}
        if self.miner_api_key:
            headers["X-API-Key"] = self.miner_api_key
        return headers
    
    async def download_file_from_proxy(self, file_url: str):
        """Download input file from proxy server"""
        try:
            bt.logging.info(f"üì• Downloading file from: {file_url}")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                headers = self._get_auth_headers()
                response = await client.get(file_url, headers=headers)
                response.raise_for_status()
                
                # Log response details for debugging
                content_type = response.headers.get("content-type", "")
                content_length = response.headers.get("content-length", "unknown")
                bt.logging.info(f"üì• Response received:")
                bt.logging.info(f"   Status: {response.status_code}")
                bt.logging.info(f"   Content-Type: {content_type}")
                bt.logging.info(f"   Content-Length: {content_length}")
                bt.logging.info(f"   Response size: {len(response.content)} bytes")
                
                # Check if we got any content
                if len(response.content) == 0:
                    bt.logging.error(f"‚ùå Downloaded file is empty (0 bytes)")
                    return None
                
                # For audio files or binary files - be more permissive
                if (content_type.startswith("audio/") or 
                    "octet-stream" in content_type or 
                    "wav" in content_type.lower() or 
                    "mp3" in content_type.lower() or
                    "m4a" in content_type.lower() or
                    "flac" in content_type.lower() or
                    file_url.lower().endswith(('.wav', '.mp3', '.m4a', '.flac'))):
                    
                    bt.logging.info(f"‚úÖ Detected audio file: {len(response.content)} bytes")
                    return response.content  # Binary audio data
                
                # For text files
                elif content_type.startswith("text/") or "json" in content_type:
                    bt.logging.info(f"‚úÖ Detected text file: {len(response.content)} bytes")
                    return response.text
                
                # Default case - treat as binary if we have content
                else:
                    bt.logging.info(f"‚ö†Ô∏è Unknown content type '{content_type}', treating as binary: {len(response.content)} bytes")
                    return response.content
                
        except httpx.HTTPStatusError as e:
            bt.logging.error(f"‚ùå HTTP error downloading file: {e.response.status_code} - {e.response.text}")
            return None
        except httpx.TimeoutException:
            bt.logging.error(f"‚ùå Timeout downloading file from {file_url}")
            return None
        except httpx.ConnectError:
            bt.logging.error(f"‚ùå Connection error downloading file from {file_url}")
            return None
        except Exception as e:
            bt.logging.error(f"‚ùå Failed to download file from {file_url}: {e}")
            return None

    async def extract_text_data(self, task_data: dict) -> Optional[str]:
        """Extract text data from task data"""
        try:
            # Try to get text directly from task data
            if "input_text" in task_data and task_data["input_text"]:
                bt.logging.info(f"‚úÖ Found input_text directly in task data")
                return task_data["input_text"]
            
            # Try to get text from input_data field
            if "input_data" in task_data and task_data["input_data"]:
                bt.logging.info(f"‚úÖ Found input_data in task data")
                return task_data["input_data"]
            
            # Try to get text from text field
            if "text" in task_data and task_data["text"]:
                bt.logging.info(f"‚úÖ Found text field in task data")
                return task_data["text"]
            
            bt.logging.warning(f"‚ö†Ô∏è No text data found in task data")
            return None
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error extracting text data: {e}")
            return None

    async def extract_summarization_data(self, task_data: dict) -> Optional[dict]:
        """Extract summarization data from task data or fetch from proxy API"""
        try:
            task_id = task_data.get("task_id")
            if not task_id:
                bt.logging.error("‚ùå No task_id provided for summarization task")
                return None
            
            # First try to get text directly from task data
            if "input_text" in task_data and task_data["input_text"]:
                bt.logging.info(f"‚úÖ Found input_text directly in task data")
                # Extract the actual text from the input_text dictionary
                text_content = task_data["input_text"]
                actual_text = text_content.get("text", "")
                source_lang = text_content.get("source_language", task_data.get("source_language", "en"))
                
                bt.logging.info(f"   Extracted text length: {len(actual_text)} characters")
                bt.logging.info(f"   Text preview: {actual_text[:100]}...")
                
                return {
                    "text": actual_text,
                    "source_language": source_lang,
                    "detected_language": source_lang,  # Use source language directly
                    "language_confidence": 1.0  # Always 1.0 since user specified the language
                }
            
            # If no direct text, fetch from proxy API
            bt.logging.info(f"üì° Fetching summarization task content from proxy API for task {task_id}")
            
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    headers = self._get_auth_headers()
                    response = await client.get(f"{self.proxy_server_url}/api/v1/miner/summarization/{task_id}", headers=headers)
                    
                    if response.status_code == 200:
                        response_data = response.json()
                        if response_data.get("success"):
                            text_content = response_data.get("text_content", {})
                            bt.logging.info(f"‚úÖ Successfully fetched summarization content from proxy API")
                            bt.logging.info(f"   Text length: {len(text_content.get('text', ''))}")
                            bt.logging.info(f"   Source language: {text_content.get('source_language', 'en')}")
                            
                            return {
                                "text": text_content.get("text", ""),
                                "source_language": text_content.get("source_language", "en"),
                                "detected_language": text_content.get("source_language", "en"),  # Use source language directly
                                "language_confidence": 1.0,  # Always 1.0 since user specified the language
                                "task_metadata": response_data.get("task_metadata", {})
                            }
                        else:
                            bt.logging.error(f"‚ùå Proxy API returned error: {response_data}")
                            return None
                    else:
                        bt.logging.error(f"‚ùå Failed to fetch summarization content: HTTP {response.status_code}")
                        return None
                        
            except Exception as e:
                bt.logging.error(f"‚ùå Error fetching summarization content from proxy API: {e}")
                return None
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error extracting summarization data: {e}")
            return None

    async def extract_tts_data(self, task_data: dict) -> Optional[dict]:
        """Extract TTS data from task data or fetch from proxy API"""
        try:
            task_id = task_data.get("task_id")
            if not task_id:
                bt.logging.error("‚ùå No task_id provided for TTS task")
                return None
            
            # First try to get text directly from task data
            if "input_text" in task_data and task_data["input_text"]:
                bt.logging.info(f"‚úÖ Found input_text directly in task data")
                # Extract the actual text from the input_text dictionary
                text_content = task_data["input_text"]
                # Handle both dict and string formats
                if isinstance(text_content, dict):
                    actual_text = text_content.get("text", "")
                    source_lang = text_content.get("source_language", task_data.get("source_language", "en"))
                elif isinstance(text_content, str):
                    actual_text = text_content
                    source_lang = task_data.get("source_language", "en")
                else:
                    actual_text = str(text_content) if text_content else ""
                    source_lang = task_data.get("source_language", "en")
                
                bt.logging.info(f"   Extracted text length: {len(actual_text)} characters")
                bt.logging.info(f"   Text preview: {actual_text[:100]}...")
                
                # Get voice information from task data
                speaker_wav_url = task_data.get("speaker_wav_url")
                voice_name = task_data.get("voice_name")
                
                # If speaker_wav_url is missing but voice_name exists, we need to fetch it
                # This can happen if the task was created before voice lookup was implemented
                if not speaker_wav_url and voice_name:
                    bt.logging.warning(f"‚ö†Ô∏è Task {task_id} has voice_name '{voice_name}' but no speaker_wav_url. Fetching from API...")
                    # Fall through to API fetch below
                elif speaker_wav_url:
                    # We have everything we need from task_data
                    voice_info = {
                        "voice_name": voice_name,
                        "speaker_wav_url": speaker_wav_url,
                        "model_id": task_data.get("model_id", "tts_models/multilingual/multi-dataset/xtts_v2")
                    }
                    
                    return {
                        "text": actual_text,
                        "source_language": source_lang,
                        "detected_language": source_lang,  # Use source language directly
                        "language_confidence": 1.0,  # Always 1.0 since user specified the language
                        "voice_info": voice_info
                    }
                else:
                    # No voice_name or speaker_wav_url - fall through to API fetch
                    bt.logging.warning(f"‚ö†Ô∏è Task {task_id} has no voice_name or speaker_wav_url. Fetching from API...")
            
            # If no direct text, fetch from proxy API
            bt.logging.info(f"üì° Fetching TTS task content from proxy API for task {task_id}")
            
            try:
                async with httpx.AsyncClient(timeout=30.0) as client:
                    headers = self._get_auth_headers()
                    response = await client.get(f"{self.proxy_server_url}/api/v1/miner/tts/{task_id}", headers=headers)
                    
                    if response.status_code == 200:
                        response_data = response.json()
                        if response_data.get("success"):
                            text_content = response_data.get("text_content", {})
                            voice_info = response_data.get("voice_info")
                            task_metadata = response_data.get("task_metadata", {})
                            
                            # Ensure text_content is a dict
                            if not isinstance(text_content, dict):
                                text_content = {}
                            
                            # Ensure voice_info is a dict (not None)
                            if voice_info is None:
                                voice_info = {}
                            elif not isinstance(voice_info, dict):
                                voice_info = {}
                            
                            # Ensure model_id is included in voice_info (from task_data or API response)
                            if not voice_info.get("model_id"):
                                voice_info["model_id"] = task_data.get("model_id", "tts_models/multilingual/multi-dataset/xtts_v2")
                            
                            bt.logging.info(f"‚úÖ Successfully fetched TTS content from proxy API")
                            bt.logging.info(f"   Text length: {len(text_content.get('text', ''))}")
                            bt.logging.info(f"   Source language: {text_content.get('source_language', 'en')}")
                            
                            # Validate that we have speaker_wav_url before returning
                            if not voice_info or not voice_info.get('speaker_wav_url'):
                                error_msg = f"Task {task_id} missing speaker_wav_url in API response. Voice name: {voice_info.get('voice_name') if voice_info else 'None'}"
                                bt.logging.error(f"‚ùå {error_msg}")
                                bt.logging.error(f"   This usually means the voice was not found in the Voice table or the task was created without a voice_name")
                                return None
                            
                            bt.logging.info(f"   Voice: {voice_info.get('voice_name')}")
                            bt.logging.info(f"   Model ID: {voice_info.get('model_id')}")
                            bt.logging.info(f"   Speaker WAV URL: {voice_info.get('speaker_wav_url')[:50]}...")
                            
                            return {
                                "text": text_content.get("text", ""),
                                "source_language": text_content.get("source_language", "en"),
                                "detected_language": text_content.get("source_language", "en"),  # Use source language directly
                                "language_confidence": 1.0,  # Always 1.0 since user specified the language
                                "voice_info": voice_info,
                                "task_metadata": task_metadata
                            }
                        else:
                            bt.logging.error(f"‚ùå Proxy API returned error: {response_data}")
                            return None
                    else:
                        bt.logging.error(f"‚ùå Failed to fetch TTS content: HTTP {response.status_code}")
                        return None
                        
            except Exception as e:
                bt.logging.error(f"‚ùå Error fetching TTS content from proxy API: {e}")
                return None
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error extracting TTS data: {e}")
            return None
    
    async def process_transcription_task(self, audio_data: bytes, model_id: Optional[str] = None, language: str = "en"):
        """Process transcription task using pipeline with specified model and language"""
        try:
            # Get pipeline with specified model (loads on-demand)
            bt.logging.info(f"üîÑ Loading transcription pipeline with model: {model_id or 'default'}")
            pipeline = self.pipeline_manager.get_transcription_pipeline(model_id)
            if pipeline is None:
                raise Exception("Transcription pipeline not available")
            
            # Validate input data
            if not isinstance(audio_data, bytes):
                raise Exception(f"Invalid audio data type: expected bytes, got {type(audio_data)}")
            
            if len(audio_data) == 0:
                raise Exception("Audio data is empty")
            
            # Validate and normalize language code
            language = language.lower() if language else "en"
            supported_languages = pipeline.language_codes.keys()
            if language not in supported_languages:
                bt.logging.warning(f"‚ö†Ô∏è Language '{language}' may not be fully supported, using anyway")
            
            bt.logging.info(f"üéµ Processing {len(audio_data)} bytes of audio data in language: {language}...")
            
            # Process audio data with specified language
            transcribed_text, processing_time = pipeline.transcribe(
                audio_data, language=language
            )
            
            bt.logging.info(f"‚úÖ Transcription completed: {len(transcribed_text)} characters in {processing_time:.2f}s")
            
            return {
                "transcript": transcribed_text,
                "confidence": 0.95,  # Mock confidence score
                "processing_time": processing_time,
                "language": language  # Return the actual language used
            }
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error processing transcription task: {e}")
            bt.logging.error(f"   Audio data type: {type(audio_data)}")
            bt.logging.error(f"   Audio data length: {len(audio_data) if hasattr(audio_data, '__len__') else 'N/A'}")
            
            # Check if this is a broken file error
            if "Audio data is empty" in str(e) or len(audio_data) == 0:
                bt.logging.warning(f"‚ö†Ô∏è Broken file detected - marking as completed with broken file notice")
                return {
                    "transcript": "broken file",
                    "confidence": 0.0,
                    "processing_time": 0.0,
                    "language": "en",
                    "error": "File is empty or broken",
                    "status": "completed_broken_file"
                }
            
            # For other errors, return standard error format
            return {
                "transcript": "",
                "confidence": 0.0,
                "processing_time": 0.0,
                "language": "en",
                "error": str(e)
            }
    
    # NOTE: The process_tts_task function that expects a string has been removed
    # because it was being overridden by the function at line 1639 that expects a dict.
    # All TTS processing now uses the Coqui TTS API with voice cloning (process_tts_task with dict parameter).
    
    async def process_summarization_task_from_proxy(self, task_data: dict):
        """Process summarization task from proxy server using text-based input"""
        try:
            task_id = task_data.get("task_id")
            bt.logging.info(f"üìù Processing summarization task {task_id} from proxy server")
            
            # Extract summarization data using the existing method
            summarization_data = await self.extract_summarization_data(task_data)
            
            if not summarization_data:
                error_msg = f"Failed to extract summarization data for task {task_id}"
                bt.logging.error(f"‚ùå {error_msg}")
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                self.log_task_completion(task_id, "summarization", miner_uid, 0.0, False, {}, error_msg)
                return
            
            # Process the summarization task
            start_time = time.time()
            result = await self.process_summarization_task(summarization_data)
            processing_time = time.time() - start_time
            
            # Validate result before submission
            if not result:
                error_msg = f"Task {task_id} returned empty result"
                bt.logging.error(f"‚ùå {error_msg}")
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                self.log_task_completion(task_id, "summarization", miner_uid, processing_time, False, result or {}, error_msg)
                return
            
            # Check if result contains an error
            if "error" in result:
                error_msg = result.get("error", "Unknown error")
                bt.logging.error(f"‚ùå Task {task_id} failed: {error_msg}")
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                self.log_task_completion(task_id, "summarization", miner_uid, processing_time, False, result, error_msg)
                return
            
            # Check if summary is empty (which indicates failure)
            if not result.get("summary", "").strip():
                error_msg = f"Task {task_id} produced empty summary"
                bt.logging.error(f"‚ùå {error_msg}")
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                self.log_task_completion(task_id, "summarization", miner_uid, processing_time, False, result, error_msg)
                return
            
            # Log successful task completion
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            self.log_task_completion(task_id, "summarization", miner_uid, processing_time, True, result)
            
            bt.logging.info(f"‚úÖ Task {task_id} processed successfully by summarization pipeline in {processing_time:.2f}s")
            
            # Submit result back to proxy
            await self.submit_result_to_proxy(f"{self.proxy_server_url}/api/v1/miner/response", task_id, result)
            
            # Log final response
            self.log_response(task_id, "summarization", miner_uid, result, processing_time, 0, True)
            
            bt.logging.info(f"‚úÖ Task {task_id} completed and result submitted")
            
        except Exception as e:
            error_msg = f"Error processing summarization task from proxy: {str(e)}"
            bt.logging.error(f"‚ùå {error_msg}")
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            self.log_response(task_id, "summarization", miner_uid, {}, 0.0, 0, False, error_msg)

    async def process_tts_task_from_proxy(self, task_data: dict):
        """Process TTS task from proxy server using text-based input"""
        try:
            task_id = task_data.get("task_id")
            bt.logging.info(f"üéµ Processing TTS task {task_id} from proxy server")
            
            # Log task data structure for debugging
            bt.logging.debug(f"üìã Task data keys: {list(task_data.keys())}")
            bt.logging.debug(f"   Task type: {task_data.get('task_type')}")
            bt.logging.debug(f"   Has input_text: {'input_text' in task_data}")
            bt.logging.debug(f"   Has voice_name: {'voice_name' in task_data}")
            bt.logging.debug(f"   Has speaker_wav_url: {'speaker_wav_url' in task_data}")
            
            # Extract TTS data using the existing method
            tts_data = await self.extract_tts_data(task_data)
            
            if not tts_data:
                error_msg = f"Failed to extract TTS data for task {task_id}"
                bt.logging.error(f"‚ùå {error_msg}")
                bt.logging.error(f"   Task data available keys: {list(task_data.keys())}")
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                self.log_task_completion(task_id, "tts", miner_uid, 0.0, False, {}, error_msg)
                return
            
            # Process the TTS task - get model_id from task_data or voice_info
            # Safely extract voice_info to avoid NoneType errors
            voice_info = tts_data.get("voice_info") or {}
            if not isinstance(voice_info, dict):
                voice_info = {}
            
            # Ensure model_id is extracted from task_data or voice_info, with fallback
            model_id = task_data.get("model_id") or voice_info.get("model_id") or "tts_models/multilingual/multi-dataset/xtts_v2"
            
            # Update voice_info with model_id if it's missing
            if not voice_info.get("model_id"):
                voice_info["model_id"] = model_id
                tts_data["voice_info"] = voice_info
            
            # Validate text content exists in tts_data
            text_content = tts_data.get("text", "")
            if not text_content:
                error_msg = f"No text content found in TTS data for task {task_id}"
                bt.logging.error(f"‚ùå {error_msg}")
                bt.logging.error(f"   TTS data keys: {list(tts_data.keys())}")
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                self.log_task_completion(task_id, "tts", miner_uid, 0.0, False, {}, error_msg)
                return
            
            # Validate speaker_wav_url exists
            speaker_wav_url = voice_info.get("speaker_wav_url")
            if not speaker_wav_url:
                error_msg = f"No speaker_wav_url found in voice_info for task {task_id}"
                bt.logging.error(f"‚ùå {error_msg}")
                bt.logging.error(f"   Voice info keys: {list(voice_info.keys())}")
                bt.logging.error(f"   Voice name: {voice_info.get('voice_name', 'None')}")
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                self.log_task_completion(task_id, "tts", miner_uid, 0.0, False, {}, error_msg)
                return
            
            bt.logging.info(f"‚úÖ Task {task_id} has all required data:")
            bt.logging.info(f"   Text length: {len(text_content)} characters")
            bt.logging.info(f"   Voice: {voice_info.get('voice_name', 'Unknown')}")
            bt.logging.info(f"   Speaker WAV URL: {speaker_wav_url[:60]}...")
            
            # Pass tts_data (dict) to process_tts_task, not just the text string
            # The process_tts_task function expects a dict with text, source_language, voice_info, etc.
            start_time = time.time()
            result = await self.process_tts_task(tts_data, model_id=model_id)
            processing_time = time.time() - start_time
            
            # Validate result before submission
            if not result or "error" in result:
                error_msg = f"Task {task_id} failed to produce valid result: {result}"
                bt.logging.error(f"‚ùå {error_msg}")
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                self.log_task_completion(task_id, "tts", miner_uid, processing_time, False, result, error_msg)
                return
            
            # Log successful task completion
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            self.log_task_completion(task_id, "tts", miner_uid, processing_time, True, result)
            
            bt.logging.info(f"‚úÖ Task {task_id} processed successfully by TTS pipeline in {processing_time:.2f}s")
            
            # Submit result back to proxy using TTS-specific endpoint
            await self.submit_tts_result_to_proxy(f"{self.proxy_server_url}/api/v1/miner/tts/upload-audio", task_id, result)
            
            # Log final response
            self.log_response(task_id, "tts", miner_uid, result, processing_time, 0, True)
            
            bt.logging.info(f"‚úÖ Task {task_id} completed and result submitted")
            
        except Exception as e:
            error_msg = f"Error processing TTS task from proxy: {str(e)}"
            bt.logging.error(f"‚ùå {error_msg}")
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            self.log_response(task_id, "tts", miner_uid, {}, 0.0, 0, False, error_msg)

    async def process_summarization_task(self, summarization_data: dict, model_id: Optional[str] = None):
        """Process summarization task using pipeline with specified model (loads on-demand)"""
        try:
            # Get pipeline with specified model (loads on-demand)
            bt.logging.info(f"üîÑ Loading summarization pipeline with model: {model_id or 'default'}")
            pipeline = self.pipeline_manager.get_summarization_pipeline(model_id)
            if pipeline is None:
                raise Exception("Summarization pipeline not available")
            
            # Extract text and language information
            text = summarization_data.get("text", "")
            source_language = summarization_data.get("source_language", "en")
            detected_language = summarization_data.get("detected_language", "en")
            language_confidence = summarization_data.get("language_confidence", 1.0)
            
            bt.logging.info(f"üìù Processing summarization task:")
            bt.logging.info(f"   Text type: {type(text)}")
            bt.logging.info(f"   Text length: {len(text) if isinstance(text, str) else 'N/A'}")
            bt.logging.info(f"   Text preview: {str(text)[:100] if text else 'None'}...")
            bt.logging.info(f"   Source language: {source_language}")
            bt.logging.info(f"   Detected language: {detected_language}")
            bt.logging.info(f"   Language confidence: {language_confidence}")
            
            if not text:
                raise Exception("No text provided for summarization")
            
            if not isinstance(text, str):
                raise Exception(f"Text must be a string, got {type(text)}")
            
            # Use the source language directly since user specified it
            processing_language = source_language
            
            # Process text data with language support
            summary_text, processing_time = pipeline.summarize(
                text, language=processing_language
            )
            
            return {
                "summary": summary_text,
                "processing_time": processing_time,
                "text_length": len(text),
                "source_language": source_language,
                "detected_language": detected_language,
                "language_confidence": language_confidence,
                "processing_language": processing_language,
                "word_count": len(text.split()),
                "summary_length": len(summary_text),
                "compression_ratio": len(summary_text) / max(len(text), 1)
            }
            
        except Exception as e:
            error_msg = str(e)
            bt.logging.error(f"‚ùå Error processing summarization task: {error_msg}")
            import traceback
            bt.logging.debug(f"   Full traceback: {traceback.format_exc()}")
            
            # Return error dict with proper structure
            error_result = {
                "summary": "",
                "processing_time": 0.0,
                "text_length": summarization_data.get("text", "").__len__() if isinstance(summarization_data.get("text", ""), str) else 0,
                "source_language": summarization_data.get("source_language", "en"),
                "detected_language": summarization_data.get("detected_language", "en"),
                "language_confidence": summarization_data.get("language_confidence", 0.0),
                "processing_language": summarization_data.get("source_language", "en"),
                "word_count": 0,
                "summary_length": 0,
                "compression_ratio": 0.0,
                "error": error_msg
            }
            return error_result

    async def process_tts_task(self, tts_data: dict, model_id: Optional[str] = None):
        """Process TTS task using new Coqui TTS API with speaker cloning"""
        try:
            import tempfile
            import os
            import httpx
            import sys  # Import sys explicitly to avoid scoping issues
            
            # IMPORTANT: Create LogitsWarper compatibility shim BEFORE any TTS imports
            # This must happen before TTS imports its internal modules that try to import LogitsWarper
            try:
                from transformers import LogitsWarper
                # Already available, no shim needed
            except ImportError:
                # LogitsWarper was removed in transformers 4.40.0+, create a compatibility shim
                import transformers
                import torch
                
                class LogitsWarper:
                    """Compatibility shim for LogitsWarper (removed in transformers 4.40.0+)"""
                    def __call__(self, input_ids, scores):
                        return scores
                
                class TypicalLogitsWarper(LogitsWarper):
                    """Compatibility shim for TypicalLogitsWarper"""
                    def __init__(self, mass=0.9, filter_value=-float("Inf"), min_tokens_to_keep=1):
                        self.mass = mass
                        self.filter_value = filter_value
                        self.min_tokens_to_keep = min_tokens_to_keep
                    
                    def __call__(self, input_ids, scores):
                        # Simplified typical sampling implementation
                        # This is a basic compatibility shim - may need refinement
                        return scores
                
                # Inject into transformers module BEFORE any TTS code runs
                # Patch both __dict__ and attribute access - this is what "from transformers import X" checks
                transformers.__dict__['LogitsWarper'] = LogitsWarper
                transformers.__dict__['TypicalLogitsWarper'] = TypicalLogitsWarper
                setattr(transformers, 'LogitsWarper', LogitsWarper)
                setattr(transformers, 'TypicalLogitsWarper', TypicalLogitsWarper)
                # Also make it available for direct import
                sys.modules['transformers'].__dict__['LogitsWarper'] = LogitsWarper
                sys.modules['transformers'].__dict__['TypicalLogitsWarper'] = TypicalLogitsWarper
                bt.logging.debug("‚úÖ Created LogitsWarper compatibility shim for transformers 4.57.3")
            
            # Python 3.12 compatibility workaround for spacy/pydantic ForwardRef issue
            python_version = sys.version_info
            is_python_312 = python_version.major == 3 and python_version.minor == 12
            
            if is_python_312:
                try:
                    # Patch ForwardRef._evaluate to handle Python 3.12 compatibility
                    import typing
                    if hasattr(typing, 'ForwardRef'):
                        original_evaluate = typing.ForwardRef._evaluate
                        def patched_evaluate(self, globalns=None, localns=None, *args, **kwargs):
                            # Python 3.12 requires recursive_guard as keyword-only argument
                            # Handle calls that don't provide it
                            if 'recursive_guard' not in kwargs:
                                # Not provided, add default empty set
                                kwargs['recursive_guard'] = set()
                            # Call original with all arguments
                            return original_evaluate(self, globalns, localns, *args, **kwargs)
                        typing.ForwardRef._evaluate = patched_evaluate
                        bt.logging.debug("‚úÖ Applied Python 3.12 ForwardRef compatibility patch")
                except Exception as patch_error:
                    bt.logging.warning(f"‚ö†Ô∏è Could not apply Python 3.12 compatibility patch: {patch_error}")
            
            from TTS.api import TTS
            
            # Extract text and language information
            text = tts_data.get("text", "")
            source_language = tts_data.get("source_language", "en")
            detected_language = tts_data.get("detected_language", "en")
            language_confidence = tts_data.get("language_confidence", 1.0)
            voice_info = tts_data.get("voice_info", {})
            
            # Get model_id from parameter, voice_info, or default fallback
            # Handle None, empty string, or missing values
            tts_model_id = None
            if model_id and str(model_id).strip():
                tts_model_id = str(model_id).strip()
            elif voice_info and voice_info.get("model_id"):
                tts_model_id = str(voice_info.get("model_id")).strip()
            
            # Default to XTTS v2 if no model_id provided
            if not tts_model_id or tts_model_id == "":
                tts_model_id = "tts_models/multilingual/multi-dataset/xtts_v2"
                bt.logging.warning(f"‚ö†Ô∏è No model_id provided, using default: {tts_model_id}")
            
            bt.logging.info(f"   Using TTS model: {tts_model_id}")
            
            speaker_wav_url = voice_info.get("speaker_wav_url") if voice_info else None
            
            bt.logging.info(f"üéµ Processing TTS task with new API:")
            bt.logging.info(f"   Text length: {len(text) if isinstance(text, str) else 'N/A'}")
            bt.logging.info(f"   Source language: {source_language}")
            bt.logging.info(f"   Model ID: {tts_model_id}")
            bt.logging.info(f"   Speaker WAV URL: {speaker_wav_url if speaker_wav_url else 'None'}")
            
            if not text:
                raise Exception("No text provided for TTS")
            
            if not isinstance(text, str):
                raise Exception(f"Text must be a string, got {type(text)}")
            
            if not speaker_wav_url:
                raise Exception("No speaker_wav_url provided for voice cloning")
            
            # Download speaker audio from R2
            bt.logging.info(f"üì• Downloading speaker audio from: {speaker_wav_url}")
            async with httpx.AsyncClient(timeout=30.0) as client:
                speaker_response = await client.get(speaker_wav_url)
                if speaker_response.status_code != 200:
                    raise Exception(f"Failed to download speaker audio: HTTP {speaker_response.status_code}")
                speaker_audio_data = speaker_response.content
            
            # Save speaker audio to temporary file
            with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as speaker_file:
                speaker_file.write(speaker_audio_data)
                speaker_wav_path = speaker_file.name
            
            bt.logging.info(f"‚úÖ Speaker audio downloaded: {len(speaker_audio_data)} bytes")
            
            # Detect device (GPU or CPU)
            try:
                import torch
                device = "cuda" if torch.cuda.is_available() else "cpu"
            except ImportError:
                device = "cpu"
            
            bt.logging.info(f"üîÑ Initializing TTS with model: {tts_model_id}")
            bt.logging.info(f"   Device: {device}")
            start_time = time.time()
            
            # Check transformers version (for logging only - compatibility shims are in place)
            try:
                import transformers
                transformers_version = transformers.__version__
                bt.logging.info(f"   Transformers version: {transformers_version}")
                bt.logging.debug(f"   Compatibility shims enabled for transformers 4.57.3+")
            except Exception as e:
                bt.logging.debug(f"   Could not check transformers version: {e}")
            
            # Try to use device parameter, fallback to gpu parameter
            try:
                import inspect
                sig = inspect.signature(TTS.__init__)
                if 'device' in sig.parameters:
                    tts = TTS(tts_model_id, device=device)
                else:
                    # Fallback to gpu parameter if device not available
                    tts = TTS(tts_model_id, gpu=(device == "cuda"))
            except ImportError as import_error:
                # Handle import errors (like LogitsWarper)
                error_msg = str(import_error)
                if "LogitsWarper" in error_msg or "transformers" in error_msg.lower():
                    bt.logging.error(f"‚ùå Transformers compatibility error: {error_msg}")
                    bt.logging.error(f"   This is likely due to transformers version incompatibility with Coqui TTS")
                    bt.logging.error(f"   Solution: Downgrade transformers to <4.40.0: pip install 'transformers<4.40.0'")
                    raise Exception(f"Transformers version incompatibility: {error_msg}. Please downgrade transformers to <4.40.0")
                else:
                    raise
            except TypeError as type_error:
                # Handle Python 3.12 compatibility errors
                error_msg = str(type_error)
                if "ForwardRef._evaluate()" in error_msg or "recursive_guard" in error_msg:
                    bt.logging.error(f"‚ùå Python 3.12 compatibility error: {error_msg}")
                    bt.logging.error(f"   This is a known issue with Python 3.12 + spacy/pydantic")
                    bt.logging.error(f"   Solutions:")
                    bt.logging.error(f"     1. Use Python 3.11 instead: python3.11 -m venv venv311")
                    bt.logging.error(f"     2. Or upgrade pydantic to v2 (may break other dependencies)")
                    bt.logging.error(f"     3. Or wait for spacy/pydantic updates")
                    raise Exception(f"Python 3.12 compatibility error: {error_msg}. Consider using Python 3.11 for TTS tasks.")
                else:
                    raise
            except Exception as e:
                # Final fallback
                error_msg = str(e)
                if "LogitsWarper" in error_msg or "transformers" in error_msg.lower():
                    bt.logging.error(f"‚ùå Transformers compatibility error during TTS initialization: {error_msg}")
                    bt.logging.error(f"   Solution: Downgrade transformers: pip install 'transformers<4.40.0'")
                    raise Exception(f"Transformers version incompatibility: {error_msg}. Please downgrade transformers to <4.40.0")
                elif "ForwardRef._evaluate()" in error_msg or "recursive_guard" in error_msg:
                    bt.logging.error(f"‚ùå Python 3.12 compatibility error: {error_msg}")
                    bt.logging.error(f"   Solution: Use Python 3.11 for TTS tasks")
                    raise Exception(f"Python 3.12 compatibility error: {error_msg}. Consider using Python 3.11.")
                bt.logging.warning(f"‚ö†Ô∏è Could not use device parameter, using gpu: {e}")
                try:
                    tts = TTS(tts_model_id, gpu=(device == "cuda"))
                except Exception as e2:
                    error_msg2 = str(e2)
                    if "ForwardRef._evaluate()" in error_msg2 or "recursive_guard" in error_msg2:
                        bt.logging.error(f"‚ùå Python 3.12 compatibility error in fallback: {error_msg2}")
                        raise Exception(f"Python 3.12 compatibility error: {error_msg2}. Consider using Python 3.11.")
                    raise
            
            init_time = time.time() - start_time
            bt.logging.info(f"‚úÖ TTS initialized in {init_time:.2f}s on {device}")
            
            # Note: is_multi_speaker is a read-only property in TTS library
            # We don't need to set it - XTTS v2 is multi-speaker by default
            # The library will handle this internally when we call tts_to_file with speaker_wav
            
            # Generate speech with voice cloning
            # Use try/finally to ensure TTS object and memory are cleaned up
            output_path = None
            audio_data = None
            processing_time = 0.0  # Initialize to avoid NameError if synthesis fails
            try:
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as output_file:
                    output_path = output_file.name
                
                bt.logging.info(f"üîä Generating speech with voice cloning...")
                synthesis_start = time.time()
                tts.tts_to_file(
                    text=text,
                    file_path=output_path,
                    speaker_wav=speaker_wav_path,
                    language=source_language
                )
                processing_time = time.time() - synthesis_start
                
                # Read generated audio
                with open(output_path, 'rb') as f:
                    audio_data = f.read()
                
                bt.logging.info(f"‚úÖ Speech generated: {len(audio_data)} bytes in {processing_time:.2f}s")
            finally:
                # Critical: Clean up TTS object and free memory to prevent memory corruption
                try:
                    # Delete TTS object explicitly
                    del tts
                    tts = None
                    
                    # Clear PyTorch cache if using GPU
                    if device == "cuda":
                        try:
                            import torch
                            torch.cuda.empty_cache()
                            torch.cuda.synchronize()
                            bt.logging.debug("   Cleared CUDA cache")
                        except Exception as e:
                            bt.logging.debug(f"   Could not clear CUDA cache: {e}")
                    
                    # Force garbage collection
                    import gc
                    gc.collect()
                    bt.logging.debug("   Forced garbage collection")
                except Exception as cleanup_error:
                    bt.logging.warning(f"‚ö†Ô∏è Error during TTS cleanup: {cleanup_error}")
            
            # Cleanup temporary files
            try:
                if speaker_wav_path and os.path.exists(speaker_wav_path):
                    os.unlink(speaker_wav_path)
                if output_path and os.path.exists(output_path):
                    os.unlink(output_path)
            except Exception as file_cleanup_error:
                bt.logging.debug(f"   Could not clean up temp files: {file_cleanup_error}")
            
            # Generate unique filename for the audio
            import uuid
            audio_filename = f"{uuid.uuid4().hex[:8]}.wav"
            
            # Store audio in Firebase Cloud Storage instead of local storage
            try:
                # Import Firebase storage manager
                import sys
                import os
                sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
                
                from proxy_server.managers.file_manager import FileManager
                from proxy_server.database.schema import DatabaseManager
                
                # Initialize database and file manager
                db_manager = DatabaseManager("proxy_server/db/violet.json")
                db_manager.initialize()
                file_manager = FileManager(db_manager.get_db())
                
                # Upload audio to Firebase Cloud Storage
                file_id = await file_manager.upload_file(
                    audio_data,
                    audio_filename,
                    "audio/wav",
                    file_type="tts"
                )
                
                # Get file metadata
                file_metadata = await file_manager.get_file_metadata(file_id)
                
                # Get public URL from metadata
                public_url = file_metadata.get('public_url') if file_metadata else None
                
                bt.logging.info(f"‚úÖ Audio file uploaded to R2: {file_id}")
                bt.logging.info(f"   File size: {len(audio_data)} bytes")
                if public_url:
                    bt.logging.info(f"   Public URL: {public_url}")
                
                return {
                    "audio_file": {
                        "file_id": file_id,
                        "filename": audio_filename,
                        "file_size": len(audio_data),
                        "file_type": "audio/wav",
                        "public_url": public_url,  # R2 public URL
                        "storage_location": "r2"
                    },
                    "processing_time": processing_time + init_time,  # Include initialization time
                    "text_length": len(text),
                    "source_language": source_language,
                    "detected_language": detected_language,
                    "language_confidence": language_confidence,
                    "processing_language": source_language,
                    "word_count": len(text.split()),
                    "model_id": tts_model_id,
                    "voice_name": voice_info.get("voice_name"),
                    "audio_duration": 0.0,  # Will be calculated by validator
                    "sample_rate": 22050,   # Default, will be verified by validator
                    "bit_depth": 16,        # Default, will be verified by validator
                    "channels": 1           # Default, will be verified by validator
                }
                
            except Exception as storage_error:
                bt.logging.error(f"‚ùå Failed to upload to Firebase Cloud Storage: {storage_error}")
                # Fallback to local storage if Firebase fails
                audio_path = f"proxy_server/local_storage/tts_audio/{audio_filename}"
                os.makedirs(os.path.dirname(audio_path), exist_ok=True)
                
                with open(audio_path, "wb") as f:
                    f.write(audio_data)
                
                bt.logging.info(f"‚úÖ Audio file saved locally (fallback): {audio_path}")
                bt.logging.info(f"   File size: {len(audio_data)} bytes")
                
                return {
                    "audio_file": {
                        "filename": audio_filename,
                        "local_path": audio_path,
                        "file_size": len(audio_data),
                        "file_type": "audio/wav"
                    },
                    "processing_time": processing_time,
                    "text_length": len(text),
                    "source_language": source_language,
                    "detected_language": detected_language,
                    "language_confidence": language_confidence,
                    "processing_language": processing_language,
                    "word_count": len(text.split()),
                    "audio_duration": 0.0,  # Will be calculated by validator
                    "sample_rate": 22050,   # Default, will be verified by validator
                    "bit_depth": 16,        # Default, will be verified by validator
                    "channels": 1           # Default, will be verified by validator
                }
            
        except Exception as e:
            error_msg = str(e)
            bt.logging.error(f"‚ùå Error processing TTS task: {e}")
            
            # Provide specific guidance based on error type
            if "LogitsWarper" in error_msg:
                bt.logging.error(f"   This is a transformers version issue. Run: pip install 'transformers<4.40.0'")
            elif "ForwardRef._evaluate()" in error_msg or "recursive_guard" in error_msg:
                bt.logging.error(f"   This is a Python 3.12 compatibility issue with spacy/pydantic.")
                bt.logging.error(f"   Solution: Use Python 3.11 for TTS tasks: python3.11 -m venv venv311")
                bt.logging.error(f"   Or run: ./fix_python312_tts.sh for more options")
            
            return {
                "audio_file": None,
                "processing_time": 0.0,
                "text_length": 0,
                "source_language": "en",
                "detected_language": "en",
                "language_confidence": 0.0,
                "processing_language": "en",
                "word_count": 0,
                "audio_duration": 0.0,
                "sample_rate": 0,
                "bit_depth": 0,
                "channels": 0,
                "error": error_msg
            }
    
    async def process_video_transcription_task(self, video_data: bytes, task_data: dict, model_id: Optional[str] = None):
        """Process video transcription task - extract audio and transcribe"""
        try:
            # Get model_id from task_data if not provided
            if model_id is None:
                model_id = task_data.get("model_id")
            
            # Get pipeline with specified model (loads on-demand)
            bt.logging.info(f"üîÑ Loading transcription pipeline with model: {model_id or 'default'} for video transcription")
            pipeline = self.pipeline_manager.get_transcription_pipeline(model_id)
            if pipeline is None:
                raise Exception("Transcription pipeline not available")
            
            # Import video processing utilities
            try:
                from template.pipelines.video_utils import video_processor
            except ImportError:
                raise Exception("Video processing utilities not available")
            
            # Get task information
            task_id = task_data.get("task_id", "unknown")
            source_language = task_data.get("source_language", "en")
            filename = task_data.get("input_file", {}).get("file_name", "unknown_video")
            
            bt.logging.info(f"üé¨ Processing video transcription task {task_id}")
            bt.logging.info(f"   Video filename: {filename}")
            bt.logging.info(f"   Video size: {len(video_data)} bytes")
            bt.logging.info(f"   Source language: {source_language}")
            
            # Extract audio from video
            bt.logging.info(f"üîß Extracting audio from video...")
            audio_bytes, temp_audio_path = video_processor.extract_audio_from_video(
                video_data, 
                filename,
                output_format="wav",
                sample_rate=16000  # Whisper requirement
            )
            
            bt.logging.info(f"‚úÖ Audio extraction successful: {len(audio_bytes)} bytes")
            
            # Get video information for metadata
            video_info = video_processor.get_video_info(video_data, filename)
            bt.logging.info(f"üìä Video info: {video_info}")
            
            # Transcribe the extracted audio
            bt.logging.info(f"üéµ Transcribing extracted audio...")
            transcribed_text, processing_time = pipeline.transcribe(
                audio_bytes, language=source_language
            )
            
            bt.logging.info(f"‚úÖ Transcription completed: {len(transcribed_text)} characters in {processing_time:.2f}s")
            
            # Calculate confidence score (mock for now)
            confidence = 0.95
            
            return {
                "transcript": transcribed_text,
                "confidence": confidence,
                "processing_time": processing_time,
                "language": source_language,
                "video_info": video_info,
                "audio_extraction_success": True,
                "audio_size_bytes": len(audio_bytes),
                "transcript_length": len(transcribed_text),
                "word_count": len(transcribed_text.split()),
                "source_language": source_language
            }
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error processing video transcription task: {e}")
            return {
                "transcript": "",
                "confidence": 0.0,
                "processing_time": 0.0,
                "language": source_language if 'source_language' in locals() else "en",
                "video_info": {},
                "audio_extraction_success": False,
                "audio_size_bytes": 0,
                "transcript_length": 0,
                "word_count": 0,
                "source_language": source_language if 'source_language' in locals() else "en",
                "error": str(e)
            }
    
    async def process_text_translation_task(self, task_data: dict, model_id: Optional[str] = None):
        """Process text translation task from proxy server"""
        try:
            task_id = task_data.get("task_id")
            # Get model_id from task_data if not provided
            if model_id is None:
                model_id = task_data.get("model_id")
            
            bt.logging.info(f"üåê Processing text translation task {task_id} from proxy server")
            
            # Extract translation data
            translation_data = await self.extract_text_translation_data(task_data)
            
            if not translation_data:
                error_msg = f"Failed to extract text translation data for task {task_id}"
                bt.logging.error(f"‚ùå {error_msg}")
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                self.log_task_completion(task_id, "text_translation", miner_uid, 0.0, False, {}, error_msg)
                return
            
            # Process the text translation task
            start_time = time.time()
            result = await self.process_text_translation(translation_data, model_id=model_id)
            processing_time = time.time() - start_time
            
            # Validate result before submission
            if not result or "error" in result:
                error_msg = f"Task {task_id} failed to produce valid result: {result}"
                bt.logging.error(f"‚ùå {error_msg}")
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                self.log_task_completion(task_id, "text_translation", miner_uid, processing_time, False, result, error_msg)
                return
            
            # Log successful task completion
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            self.log_task_completion(task_id, "text_translation", miner_uid, processing_time, True, result)
            
            bt.logging.info(f"‚úÖ Task {task_id} processed successfully by text translation pipeline in {processing_time:.2f}s")
            
            # Submit result back to proxy using text translation-specific endpoint
            await self.submit_text_translation_result_to_proxy(f"{self.proxy_server_url}/api/v1/miner/text-translation/upload-result", task_id, result)
            
            # Log final response
            self.log_response(task_id, "text_translation", miner_uid, result, processing_time, 0, True)
            
            bt.logging.info(f"‚úÖ Task {task_id} completed and result submitted")
            
        except Exception as e:
            error_msg = f"Error processing text translation task from proxy: {str(e)}"
            bt.logging.error(f"‚ùå {error_msg}")
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            self.log_response(task_id, "text_translation", miner_uid, {}, 0.0, 0, False, error_msg)
    
    async def process_document_translation_task_from_proxy(self, task_data: dict):
        """Process document translation task from proxy server"""
        try:
            task_id = task_data.get("task_id")
            bt.logging.info(f"üìÑ Processing document translation task {task_id} from proxy server")
            
            # Extract document data
            if 'input_file' in task_data and task_data['input_file']:
                input_file = task_data['input_file']
                file_id = input_file.get('file_id')
                
                if not file_id:
                    bt.logging.error(f"‚ùå No file_id found in input_file for task {task_id}")
                    return
                
                # Download the document file from proxy
                input_data = await self.download_file_from_proxy(f"{self.proxy_server_url}/api/v1/files/{file_id}/download")
                
                if input_data is None:
                    bt.logging.error(f"‚ùå Failed to download document file for task {task_id}")
                    return
                
                # Now process the document translation
                await self.process_document_translation_task(input_data, task_data)
            else:
                bt.logging.error(f"‚ùå No input_file found in task data for document translation task {task_id}")
                
        except Exception as e:
            error_msg = f"Error processing document translation task from proxy: {str(e)}"
            bt.logging.error(f"‚ùå {error_msg}")
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            self.log_response(task_id, "document_translation", miner_uid, {}, 0.0, 0, False, error_msg)

    async def process_document_translation_task(self, document_data, task_data: dict, model_id: Optional[str] = None):
        """Process document translation task from proxy server"""
        try:
            task_id = task_data.get("task_id")
            # Get model_id from task_data if not provided
            if model_id is None:
                model_id = task_data.get("model_id")
            
            bt.logging.info(f"üìÑ Processing document translation task {task_id} from proxy server")
            
            # Extract translation data
            translation_data = await self.extract_document_translation_data(task_data)
            
            if not translation_data:
                error_msg = f"Failed to extract document translation data for task {task_id}"
                bt.logging.error(f"‚ùå {error_msg}")
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                self.log_task_completion(task_id, "document_translation", miner_uid, 0.0, False, {}, error_msg)
                return
            
            # Process the document translation task
            start_time = time.time()
            result = await self.process_document_translation(document_data, translation_data, model_id=model_id)
            processing_time = time.time() - start_time
            
            # Validate result before submission
            if not result or "error" in result:
                error_msg = f"Task {task_id} failed to produce valid result: {result}"
                bt.logging.error(f"‚ùå {error_msg}")
                miner_uid = self.uid if hasattr(self, 'uid') else 0
                self.log_task_completion(task_id, "document_translation", miner_uid, processing_time, False, result, error_msg)
                return
            
            # Log successful task completion
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            self.log_task_completion(task_id, "document_translation", miner_uid, processing_time, True, result)
            
            bt.logging.info(f"‚úÖ Task {task_id} processed successfully by document translation pipeline in {processing_time:.2f}s")
            
            # Submit result back to proxy using document translation-specific endpoint
            await self.submit_document_translation_result_to_proxy(f"{self.proxy_server_url}/api/v1/miner/document-translation/upload-result", task_id, result)
            
            # Log final response
            self.log_response(task_id, "document_translation", miner_uid, result, processing_time, len(document_data), True)
            
            bt.logging.info(f"‚úÖ Task {task_id} completed and result submitted")
            
        except Exception as e:
            error_msg = f"Error processing document translation task from proxy: {str(e)}"
            bt.logging.error(f"‚ùå {error_msg}")
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            self.log_response(task_id, "document_translation", miner_uid, {}, 0.0, 0, False, error_msg)
    
    async def extract_text_translation_data(self, task_data: dict) -> Optional[dict]:
        """Extract text translation data from task data"""
        try:
            task_id = task_data.get("task_id")
            
            # Try to get data from task_data first
            if 'input_text' in task_data and task_data['input_text']:
                text_content = task_data['input_text']
                bt.logging.info(f"‚úÖ Found text translation data in task_data for task {task_id}")
                return {
                    'text': text_content.get('text', ''),
                    'source_language': text_content.get('source_language', 'en'),
                    'target_language': text_content.get('target_language', 'es')
                }
            
            # If not in task_data, try to fetch from proxy API
            bt.logging.info(f"üîç Fetching text translation data from proxy API for task {task_id}")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                headers = self._get_auth_headers()
                response = await client.get(f"{self.proxy_server_url}/api/v1/miner/text-translation/{task_id}", headers=headers)
                
                if response.status_code == 200:
                    data = response.json()
                    if data.get('success') and 'text_content' in data:
                        text_content = data['text_content']
                        bt.logging.info(f"‚úÖ Retrieved text translation data from proxy API for task {task_id}")
                        return {
                            'text': text_content.get('text', ''),
                            'source_language': text_content.get('source_language', 'en'),
                            'target_language': text_content.get('target_language', 'es')
                        }
                    else:
                        bt.logging.warning(f"‚ö†Ô∏è No text content found in proxy API response for task {task_id}")
                        return None
                else:
                    bt.logging.error(f"‚ùå Failed to fetch text translation data from proxy API: {response.status_code}")
                    return None
                        
        except Exception as e:
            bt.logging.error(f"‚ùå Error extracting text translation data: {e}")
            return None
    
    async def extract_document_translation_data(self, task_data: dict) -> Optional[dict]:
        """Extract document translation data from task data"""
        try:
            task_id = task_data.get("task_id")
            
            # Try to get data from task_data first
            if 'input_file' in task_data and task_data['input_file']:
                input_file = task_data['input_file']
                bt.logging.info(f"‚úÖ Found document translation data in task_data for task {task_id}")
                return {
                    'filename': input_file.get('file_name', 'unknown'),
                    'source_language': task_data.get('source_language', 'en'),
                    'target_language': task_data.get('target_language', 'es')
                }
            
            # If not in task_data, try to fetch from proxy API
            bt.logging.info(f"üîç Fetching document translation data from proxy API for task {task_id}")
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                headers = self._get_auth_headers()
                response = await client.get(f"{self.proxy_server_url}/api/v1/miner/document-translation/{task_id}", headers=headers)
                
                if response.status_code == 200:
                    data = response.json()
                    if data.get('success') and 'file_metadata' in data:
                        file_metadata = data['file_metadata']
                        bt.logging.info(f"‚úÖ Retrieved document translation data from proxy API for task {task_id}")
                        return {
                            'filename': file_metadata.get('file_name', 'unknown'),
                            'source_language': data['task_metadata'].get('source_language', 'en'),
                            'target_language': data['task_metadata'].get('target_language', 'es')
                        }
                    else:
                        bt.logging.warning(f"‚ö†Ô∏è No file metadata found in proxy API response for task {task_id}")
                        return None
                else:
                    bt.logging.error(f"‚ùå Failed to fetch document translation data from proxy API: {response.status_code}")
                    return None
                        
        except Exception as e:
            bt.logging.error(f"‚ùå Error extracting document translation data: {e}")
            return None
    
    async def process_text_translation(self, translation_data: dict, model_id: Optional[str] = None) -> dict:
        """Process text translation using translation pipeline with specified model or default"""
        try:
            # Get pipeline with specified model (loads on-demand)
            bt.logging.info(f"üîÑ Loading translation pipeline with model: {model_id or 'default'} for text translation")
            pipeline = self.pipeline_manager.get_translation_pipeline(model_id)
            if pipeline is None:
                raise Exception("Translation pipeline not available")
            
            # Extract translation parameters
            text = translation_data.get("text", "")
            source_language = translation_data.get("source_language", "en")
            target_language = translation_data.get("target_language", "es")
            
            bt.logging.info(f"üåê Processing text translation:")
            bt.logging.info(f"   Text length: {len(text)} characters")
            bt.logging.info(f"   From {source_language} to {target_language}")
            
            if not text:
                raise Exception("No text provided for translation")
            
            # Process text translation
            translated_text, processing_time = pipeline.translate_text(
                text, source_language, target_language
            )
            
            return {
                "translated_text": translated_text,
                "source_language": source_language,
                "target_language": target_language,
                "processing_time": processing_time,
                "original_text_length": len(text),
                "translated_text_length": len(translated_text),
                "word_count": len(text.split()),
                "translated_word_count": len(translated_text.split())
            }
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error processing text translation: {e}")
            return {
                "translated_text": "",
                "source_language": translation_data.get("source_language", "en"),
                "target_language": translation_data.get("target_language", "es"),
                "processing_time": 0.0,
                "original_text_length": 0,
                "translated_text_length": 0,
                "word_count": 0,
                "translated_word_count": 0,
                "error": str(e)
            }
    
    async def process_document_translation(self, document_data, translation_data: dict, model_id: Optional[str] = None) -> dict:
        """Process document translation using translation pipeline with specified model or default"""
        try:
            # Get pipeline with specified model (loads on-demand)
            bt.logging.info(f"üîÑ Loading translation pipeline with model: {model_id or 'default'} for document translation")
            pipeline = self.pipeline_manager.get_translation_pipeline(model_id)
            if pipeline is None:
                raise Exception("Translation pipeline not available")
            
            # Extract translation parameters
            filename = translation_data.get("filename", "unknown")
            source_language = translation_data.get("source_language", "en")
            target_language = translation_data.get("target_language", "es")
            
            bt.logging.info(f"üìÑ Processing document translation:")
            bt.logging.info(f"   Filename: {filename}")
            bt.logging.info(f"   File size: {len(document_data)} bytes")
            bt.logging.info(f"   From {source_language} to {target_language}")
            
            if not document_data:
                raise Exception("No document data provided for translation")
            
            # Process document translation
            translated_text, processing_time, metadata = pipeline.translate_document(
                document_data, filename, source_language, target_language
            )
            
            return {
                "translated_text": translated_text,
                "source_language": source_language,
                "target_language": target_language,
                "processing_time": processing_time,
                "original_filename": filename,
                "file_size_bytes": len(document_data),
                "translated_text_length": len(translated_text),
                "metadata": metadata
            }
            
        except Exception as e:
            bt.logging.error(f"‚ùå Error processing document translation: {e}")
            return {
                "translated_text": "",
                "source_language": translation_data.get("source_language", "en"),
                "target_language": translation_data.get("target_language", "es"),
                "processing_time": 0.0,
                "original_filename": translation_data.get("filename", "unknown"),
                "file_size_bytes": len(document_data) if document_data else 0,
                "translated_text_length": 0,
                "metadata": {},
                "error": str(e)
            }
    
    async def submit_text_translation_result_to_proxy(self, callback_url: str, task_id: str, result: dict):
        """Submit text translation result to proxy server"""
        try:
            # Get miner UID from Bittensor
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            
            # Prepare form data
            form_data = {
                'task_id': task_id,
                'miner_uid': miner_uid,
                'translated_text': result.get('translated_text', ''),
                'processing_time': result.get('processing_time', 0.0),
                'accuracy_score': 0.95,  # Mock confidence for translation
                'speed_score': max(0.5, 1.0 - (result.get('processing_time', 0.0) / 10.0)),
                'source_language': result.get('source_language', 'en'),
                'target_language': result.get('target_language', 'es')
            }
            
            bt.logging.info(f"üì§ Submitting text translation result to proxy server for task {task_id}")
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                headers = self._get_auth_headers()
                response = await client.post(callback_url, headers=headers, data=form_data)
                
                if response.status_code == 200:
                    bt.logging.info(f"‚úÖ Text translation result submitted successfully for task {task_id}")
                else:
                    bt.logging.warning(f"‚ö†Ô∏è Failed to submit text translation result: {response.status_code}")
                    
        except Exception as e:
            bt.logging.error(f"‚ùå Error submitting text translation result: {e}")
    
    async def submit_document_translation_result_to_proxy(self, callback_url: str, task_id: str, result: dict):
        """Submit document translation result to proxy server"""
        try:
            # Get miner UID from Bittensor
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            
            # Prepare form data
            form_data = {
                'task_id': task_id,
                'miner_uid': miner_uid,
                'translated_text': result.get('translated_text', ''),
                'processing_time': result.get('processing_time', 0.0),
                'accuracy_score': 0.95,  # Mock confidence for translation
                'speed_score': max(0.5, 1.0 - (result.get('processing_time', 0.0) / 10.0)),
                'source_language': result.get('source_language', 'en'),
                'target_language': result.get('target_language', 'es'),
                'metadata': json.dumps(result.get('metadata', {}))
            }
            
            bt.logging.info(f"üì§ Submitting document translation result to proxy server for task {task_id}")
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                headers = self._get_auth_headers()
                response = await client.post(callback_url, headers=headers, data=form_data)
                
                if response.status_code == 200:
                    bt.logging.info(f"‚úÖ Document translation result submitted successfully for task {task_id}")
                else:
                    bt.logging.warning(f"‚ö†Ô∏è Failed to submit document translation result: {response.status_code}")
                    
        except Exception as e:
            bt.logging.error(f"‚ùå Error submitting document translation result: {e}")
    
    async def submit_result_to_proxy(self, callback_url: str, task_id: str, result: dict):
        """Submit task result back to proxy server"""
        try:
            # Get miner UID from Bittensor
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            
            # Prepare response payload based on task type
            response_payload = {
                "task_id": task_id,
                "miner_uid": miner_uid,
                "response_data": result,
                "processing_time": result.get("processing_time", 0.0),
                "speed_score": self.calculate_speed_score(result.get("processing_time", 0.0))
            }
            
            # Add task-specific metrics
            if "confidence" in result:  # Transcription or video transcription task
                response_payload["accuracy_score"] = result.get("confidence", 0.0)
            elif "summary" in result:  # Summarization task
                response_payload["accuracy_score"] = 0.95  # Mock confidence for summarization
            elif "output_data" in result:  # TTS task
                response_payload["accuracy_score"] = 0.90  # Mock confidence for TTS
            elif "translated_text" in result:  # Translation tasks
                response_payload["accuracy_score"] = 0.95  # Mock confidence for translation
            else:
                response_payload["accuracy_score"] = 0.0
            
            bt.logging.info(f"üì§ Submitting result to proxy server for task {task_id}")
            bt.logging.info(f"   Callback URL: {callback_url}")
            bt.logging.info(f"   Miner UID: {miner_uid}")
            bt.logging.info(f"   Processing Time: {response_payload['processing_time']:.2f}s")
            bt.logging.info(f"   Accuracy Score: {response_payload['accuracy_score']:.2f}")
            bt.logging.info(f"   Speed Score: {response_payload['speed_score']:.2f}")
            
            async with httpx.AsyncClient(timeout=10.0) as client:
                headers = self._get_auth_headers()
                # Convert to Form data as expected by proxy
                form_data = {
                    'task_id': response_payload['task_id'],
                    'miner_uid': response_payload['miner_uid'],
                    'response_data': json.dumps(response_payload['response_data']),  # Use JSON serialization instead of str()
                    'processing_time': response_payload['processing_time'],
                    'accuracy_score': response_payload['accuracy_score'],
                    'speed_score': response_payload['speed_score']
                }
                
                submit_start_time = time.time()
                response = await client.post(callback_url, headers=headers, data=form_data)
                submit_time = time.time() - submit_start_time
                
                if response.status_code == 200:
                    bt.logging.info(f"‚úÖ Result submitted successfully for task {task_id}")
                    bt.logging.info(f"   Submission time: {submit_time:.2f}s")
                    bt.logging.info(f"   Response status: {response.status_code}")
                    
                    # Log submission success
                    submission_log = {
                        "timestamp": datetime.now().isoformat(),
                        "event": "result_submission_success",
                        "task_id": task_id,
                        "miner_uid": miner_uid,
                        "submission_time": submit_time,
                        "response_status": response.status_code,
                        "response_headers": dict(response.headers)
                    }
                    
                    submission_log_file = self.response_logs_dir / f"{task_id}_{miner_uid}_submission_success.json"
                    with open(submission_log_file, 'w') as f:
                        json.dump(submission_log, f, indent=2, default=str)
                        
                else:
                    error_msg = f"Failed to submit result for task {task_id}: HTTP {response.status_code}"
                    bt.logging.warning(f"‚ö†Ô∏è {error_msg}")
                    bt.logging.warning(f"   Response body: {response.text}")
                    
                    # Log submission failure
                    submission_log = {
                        "timestamp": datetime.now().isoformat(),
                        "event": "result_submission_failure",
                        "task_id": task_id,
                        "miner_uid": miner_uid,
                        "submission_time": submit_time,
                        "response_status": response.status_code,
                        "response_body": response.text,
                        "error": error_msg
                    }
                    
                    submission_log_file = self.response_logs_dir / f"{task_id}_{miner_uid}_submission_failure.json"
                    with open(submission_log_file, 'w') as f:
                        json.dump(submission_log, f, indent=2, default=str)
                    
        except Exception as e:
            error_msg = f"Error submitting result: {str(e)}"
            bt.logging.error(f"‚ùå {error_msg}")
            
            # Log submission error
            submission_log = {
                "timestamp": datetime.now().isoformat(),
                "event": "result_submission_error",
                "task_id": task_id,
                "miner_uid": miner_uid,
                "error": error_msg,
                "callback_url": callback_url
            }
            
            submission_log_file = self.response_logs_dir / f"{task_id}_{miner_uid}_submission_error.json"
            with open(submission_log_file, 'w') as f:
                json.dump(submission_log, f, indent=2, default=str)
    
    def calculate_speed_score(self, processing_time: float) -> float:
        """Calculate speed score based on processing time"""
        try:
            # Normalize processing time (lower is better)
            # Assume 10 seconds is baseline, scale to 0-1
            baseline_time = 10.0
            if processing_time <= 0:
                return 1.0
            
            speed_score = max(0.0, min(1.0, baseline_time / processing_time))
            return speed_score
            
        except Exception as e:
            bt.logging.debug(f"‚ö†Ô∏è Error calculating speed score: {e}")
            return 0.5

    async def forward(
        self, synapse: AudioTask
    ) -> AudioTask:
        """
        Handle on-chain queries from validators (handshakes and task processing).
        This is the main entry point for on-chain communication.
        
        Args:
            synapse (AudioTask): The synapse object containing the task details.

        Returns:
            AudioTask: The synapse object with the response.
        """
        import time
        import base64
        
        start_time = time.time()
        
        # Log that we received an on-chain query (this helps debug if forward is being called)
        try:
            task_type = getattr(synapse, 'task_type', 'unknown')
            input_size = len(getattr(synapse, 'input_data', '')) if hasattr(synapse, 'input_data') and synapse.input_data else 0
            # Get source info if available
            source_hotkey = "unknown"
            if hasattr(synapse, 'dendrite') and synapse.dendrite:
                source_hotkey = getattr(synapse.dendrite, 'hotkey', 'unknown')[:16] + "..."
            bt.logging.info(f"üì® On-chain query received from {source_hotkey}: task_type={task_type}, input_size={input_size} bytes")
        except Exception as e:
            bt.logging.debug(f"‚ö†Ô∏è Error logging query info: {e}")
        
        # ALWAYS process proxy tasks when any request comes in (background task)
        if hasattr(self, 'uid') and self.uid > 0:
            try:
                # Don't await - let it run in background so we can respond quickly
                asyncio.create_task(self.query_proxy_for_tasks())
            except Exception as e:
                bt.logging.debug(f"‚ö†Ô∏è Failed to query proxy for tasks: {e}")
        
        # Check if this is a handshake/connectivity test
        # Handshake uses summarization task with small test text
        is_handshake = False
        
        try:
            # Detect handshake queries:
            # 1. Summarization task type
            # 2. Small test text (like "This is a test for handshake verification.")
            if synapse.task_type == "summarization":
                try:
                    if synapse.input_data:
                        decoded = base64.b64decode(synapse.input_data.encode('utf-8'))
                        decoded_str = decoded.decode('utf-8', errors='ignore')
                        # Check if it's the handshake test text
                        if "handshake verification" in decoded_str.lower() or len(decoded_str) < 100:
                            is_handshake = True
                            bt.logging.debug(f"ü§ù Detected handshake: text contains 'handshake verification' or is small ({len(decoded_str)} chars)")
                    else:
                        # Empty input with summarization task type is also a handshake
                        is_handshake = True
                        bt.logging.debug(f"ü§ù Detected handshake: empty input with summarization task")
                except Exception as decode_err:
                    # If decoding fails but it's summarization, treat as handshake
                    if not synapse.input_data or len(synapse.input_data) < 50:
                        is_handshake = True
                        bt.logging.debug(f"ü§ù Detected handshake: decoding failed but input is small")
        except Exception as e:
            bt.logging.warning(f"‚ö†Ô∏è Error detecting handshake: {e}")
            # If we can't determine, assume it's not a handshake and process normally
        
        # Handle handshake queries - respond quickly to prove miner is online
        if is_handshake:
            try:
                processing_time = time.time() - start_time
                bt.logging.info(f"ü§ù On-chain handshake received - responding immediately (took {processing_time:.3f}s)")
                synapse.output_data = base64.b64encode(b"handshake_ack").decode('utf-8')
                synapse.processing_time = processing_time
                synapse.pipeline_model = "handshake"
                synapse.error_message = None
                bt.logging.info(f"‚úÖ Handshake response sent successfully")
                return synapse
            except Exception as e:
                bt.logging.error(f"‚ùå Error handling handshake: {e}")
                # Still return a response even if there's an error
                synapse.output_data = base64.b64encode(b"handshake_error").decode('utf-8')
                synapse.processing_time = time.time() - start_time
                synapse.error_message = str(e)
                return synapse
        
        # For real tasks, process them on-chain
        # This handles both direct on-chain tasks and validates miner capability
        try:
            bt.logging.info(f"üéØ Processing on-chain {synapse.task_type} task...")
            
            # Decode input data
            input_bytes = base64.b64decode(synapse.input_data.encode('utf-8'))
            
            # Route to appropriate pipeline based on task type
            if synapse.task_type == "transcription":
                result = await self.process_transcription_task(input_bytes)
                if result and "transcript" in result:
                    synapse.output_data = base64.b64encode(
                        result["transcript"].encode('utf-8')
                    ).decode('utf-8')
                    synapse.processing_time = result.get("processing_time", 0.0)
                    synapse.pipeline_model = result.get("pipeline_model", "whisper")
                else:
                    synapse.output_data = ""
                    synapse.error_message = "Transcription failed"
                    synapse.processing_time = time.time() - start_time
                    
            elif synapse.task_type == "tts":
                # For TTS, input_data should be text
                text = input_bytes.decode('utf-8')
                result = await self.process_tts_task({"text": text, "language": synapse.language})
                if result and "output_data" in result:
                    synapse.output_data = result["output_data"]  # Already base64
                    synapse.processing_time = result.get("processing_time", 0.0)
                    synapse.pipeline_model = result.get("pipeline_model", "tts")
                else:
                    synapse.output_data = ""
                    synapse.error_message = "TTS failed"
                    synapse.processing_time = time.time() - start_time
                    
            elif synapse.task_type == "summarization":
                # For summarization, input_data should be text
                text = input_bytes.decode('utf-8')
                result = await self.process_summarization_task({
                    "text": text,
                    "language": synapse.language
                })
                if result and "summary" in result:
                    synapse.output_data = base64.b64encode(
                        result["summary"].encode('utf-8')
                    ).decode('utf-8')
                    synapse.processing_time = result.get("processing_time", 0.0)
                    synapse.pipeline_model = result.get("pipeline_model", "bart")
                else:
                    synapse.output_data = ""
                    synapse.error_message = "Summarization failed"
                    synapse.processing_time = time.time() - start_time
            else:
                # Unknown task type - return error but still respond (proves miner is online)
                synapse.output_data = ""
                synapse.error_message = f"Unknown task type: {synapse.task_type}"
                synapse.processing_time = time.time() - start_time
                synapse.pipeline_model = "unknown"
            
            # Ensure we always return a response (even if processing failed)
            if not synapse.output_data and not synapse.error_message:
                synapse.output_data = base64.b64encode(b"task_processed").decode('utf-8')
            
            return synapse
            
        except Exception as e:
            # Even on error, return a response to prove miner is online
            processing_time = time.time() - start_time
            bt.logging.error(f"‚ùå Error processing on-chain task: {e}")
            synapse.output_data = ""
            synapse.error_message = str(e)[:200]  # Limit error message length
            synapse.processing_time = processing_time
            synapse.pipeline_model = "error"
            return synapse

    async def blacklist(
        self, synapse: AudioTask
    ) -> typing.Tuple[bool, str]:
        """
        Determines whether an incoming request should be blacklisted.
        Currently allows all connections for testing purposes.

        Args:
            synapse (AudioTask): A synapse object constructed from the headers of the incoming request.

        Returns:
            Tuple[bool, str]: A tuple containing a boolean indicating whether the synapse's hotkey is blacklisted,
                            and a string providing the reason for the decision.
        """
        # Allow all connections for testing
        bt.logging.debug(f"Allowing request from hotkey {synapse.dendrite.hotkey if synapse.dendrite else 'Unknown'}")
        return False, "Allowed for testing."

    async def priority(
        self, synapse: AudioTask
    ) -> float:
        """
        Determines the priority of the incoming request.
        Currently returns a default priority for testing.

        Args:
            synapse (AudioTask): The synapse object containing the task details.

        Returns:
            float: Priority score (default 1.0 for testing)
        """
        # Return default priority for testing
        return 1.0

    def verify(
        self, synapse: AudioTask
    ) -> None:
        """
        Verifies the synapse data.
        
        Args:
            synapse (AudioTask): A synapse object containing the data to verify.
        """
        # Log the incoming synapse for debugging
        bt.logging.debug(f"üîç Verifying synapse: task_type={synapse.task_type}, language={synapse.language}, input_data_length={len(synapse.input_data) if synapse.input_data else 0}")
        
        if synapse.task_type not in ["transcription", "tts", "summarization"]:
            bt.logging.warning(f"‚ùå Invalid task type: {synapse.task_type}")
            raise ValueError(f"Invalid task type: {synapse.task_type}")

        # For testing/connectivity checks, allow empty input data
        if not synapse.input_data:
            bt.logging.warning("‚ö†Ô∏è No input data provided - allowing for connectivity test")
            # Don't raise error for empty input during testing
            return

        if not synapse.language:
            bt.logging.warning("‚ö†Ô∏è No language specified - using default 'en'")
            synapse.language = "en"

    async def submit_tts_result_to_proxy(self, callback_url: str, task_id: str, result: dict):
        """Submit TTS result to proxy server with audio file upload"""
        try:
            # Get miner UID from Bittensor
            miner_uid = self.uid if hasattr(self, 'uid') else 0
            
            # Check if we have audio file data
            audio_file = result.get("audio_file")
            if not audio_file:
                bt.logging.error(f"‚ùå No audio file data in TTS result for task {task_id}")
                return False
            
            # Get audio content from Firebase Cloud Storage or local path
            audio_content = None
            file_id = audio_file.get("file_id")
            
            if file_id:
                # Try to get from Firebase Cloud Storage first
                try:
                    import sys
                    import os
                    sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
                    
                    from proxy_server.managers.file_manager import FileManager
                    from proxy_server.database.schema import DatabaseManager
                    
                    # Initialize database and file manager
                    db_manager = DatabaseManager("proxy_server/db/violet.json")
                    db_manager.initialize()
                    file_manager = FileManager(db_manager.get_db())
                    
                    # Download from Firebase Cloud Storage
                    audio_content = await file_manager.download_file(file_id)
                    
                    if audio_content:
                        bt.logging.info(f"‚úÖ Retrieved audio from Firebase Cloud Storage: {file_id}")
                    else:
                        bt.logging.warning(f"‚ö†Ô∏è Audio not found in Firebase Cloud Storage: {file_id}")
                        
                except Exception as storage_error:
                    bt.logging.warning(f"‚ö†Ô∏è Failed to retrieve from Firebase Cloud Storage: {storage_error}")
            
            # Fallback to local path if Firebase failed or not available
            if not audio_content:
                audio_path = audio_file.get("local_path")
                if not audio_path or not os.path.exists(audio_path):
                    bt.logging.error(f"‚ùå Audio file not found at {audio_path}")
                    return False
                
                with open(audio_path, "rb") as f:
                    audio_content = f.read()
                
                bt.logging.info(f"‚úÖ Retrieved audio from local storage: {audio_path}")
            
            bt.logging.info(f"üì§ Submitting TTS result to proxy server for task {task_id}")
            bt.logging.info(f"   Callback URL: {callback_url}")
            bt.logging.info(f"   Miner UID: {miner_uid}")
            bt.logging.info(f"   Audio file: {audio_file.get('filename')}")
            bt.logging.info(f"   File size: {len(audio_content)} bytes")
            
            # Prepare form data for TTS upload
            async with httpx.AsyncClient(timeout=30.0) as client:
                # Prepare form data
                files = {
                    'audio_file': (audio_file.get('filename'), audio_content, 'audio/wav')
                }
                
                data = {
                    'task_id': task_id,
                    'miner_uid': miner_uid,
                    'processing_time': result.get('processing_time', 0.0),
                    'accuracy_score': 0.90,  # Mock confidence for TTS
                    'speed_score': self.calculate_speed_score(result.get('processing_time', 0.0))
                }
                
                submit_start_time = time.time()
                headers = self._get_auth_headers()
                response = await client.post(callback_url, headers=headers, files=files, data=data)
                submit_time = time.time() - submit_start_time
                
                if response.status_code == 200:
                    bt.logging.info(f"‚úÖ TTS result submitted successfully for task {task_id}")
                    bt.logging.info(f"   Submission time: {submit_time:.2f}s")
                    bt.logging.info(f"   Response status: {response.status_code}")
                    
                    # Parse response to get file URL
                    response_data = response.json()
                    if response_data.get("success"):
                        file_url = response_data.get("file_url")
                        bt.logging.info(f"   Audio file URL: {file_url}")
                    
                    return True
                else:
                    error_msg = f"Failed to submit TTS result for task {task_id}: HTTP {response.status_code}"
                    bt.logging.warning(f"‚ö†Ô∏è {error_msg}")
                    bt.logging.warning(f"   Response body: {response.text}")
                    return False
                    
        except Exception as e:
            error_msg = f"Error submitting TTS result: {str(e)}"
            bt.logging.error(f"‚ùå {error_msg}")
            return False

    def cleanup_processed_tasks(self):
        """Clean up old processed tasks to prevent memory bloat"""
        try:
            if len(self.processed_tasks) > self.max_processed_tasks:
                # Keep only the most recent 80% of processed tasks
                keep_count = int(self.max_processed_tasks * 0.8)
                current_count = len(self.processed_tasks)
                
                # Convert to list, keep the last keep_count items
                tasks_list = list(self.processed_tasks)
                self.processed_tasks = set(tasks_list[-keep_count:])
                
                bt.logging.info(f"üßπ Cleaned up processed tasks: {current_count} -> {len(self.processed_tasks)}")
        except Exception as e:
            bt.logging.warning(f"‚ö†Ô∏è Error cleaning up processed tasks: {e}")
    
    def get_duplicate_protection_stats(self):
        """Get statistics about duplicate protection"""
        try:
            # Calculate protection effectiveness
            total_processed = len(self.processed_tasks)
            currently_processing = len(self.processing_tasks)
            
            # Estimate duplicate attempts prevented
            # This is based on the fact that without protection, miners would process tasks multiple times
            estimated_duplicates_prevented = total_processed * 2  # Conservative estimate
            
            return {
                'processed_tasks_count': total_processed,
                'processing_tasks_count': currently_processing,
                'max_processed_tasks': self.max_processed_tasks,
                'duplicate_protection_active': True,
                'protection_mechanisms': [
                    'In-memory processed tasks tracking',
                    'Currently processing tasks tracking',
                    'Task status validation',
                    'Atomic task processing with threading locks',
                    'Automatic cleanup of old processed tasks'
                ],
                'estimated_duplicates_prevented': estimated_duplicates_prevented,
                'memory_usage_percentage': f"{(total_processed / self.max_processed_tasks) * 100:.1f}%",
                'cleanup_threshold': f"{(self.max_processed_tasks * 0.8):.0f}",
                'thread_safety': 'enabled' if hasattr(self, 'task_processing_lock') else 'disabled'
            }
        except Exception as e:
            return {
                'error': f'Error getting duplicate protection stats: {str(e)}',
                'duplicate_protection_active': False
            }


# This is the main function, which runs the miner.
if __name__ == "__main__":
    with Miner() as miner:
        last_task_check = 0
        task_check_interval = 10  # Check for tasks every 10 seconds
        
        # Run initial tests (optional - will skip gracefully if no data available)
        try:
            import asyncio
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                # Test file download capability (will skip gracefully if no files available)
                loop.run_until_complete(miner.test_file_download())
                
                # Test miner tasks endpoint to debug task structure
                loop.run_until_complete(miner.test_miner_tasks_endpoint())
            finally:
                loop.close()
        except Exception as e:
            # Don't log as warning - tests are optional and failures are expected
            bt.logging.debug(f"Initial tests skipped: {e}")
        
        while True:
            current_time = time.time()
            
            # Check for proxy tasks periodically
            if current_time - last_task_check >= task_check_interval:
                try:
                    # Create a new event loop for task processing
                    import asyncio
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        loop.run_until_complete(miner.query_proxy_for_tasks())
                    finally:
                        loop.close()
                    last_task_check = current_time
                except Exception as e:
                    bt.logging.warning(f"‚ö†Ô∏è Main loop task check error: {e}")
            
            # Log less frequently to reduce console spam
            if int(current_time) % 60 == 0:  # Only log every minute
                bt.logging.info(f"Miner running... {current_time}")
            
            time.sleep(5)
